\documentclass{article}
\usepackage{color,graphicx,epstopdf}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{xparse}
\usepackage
[
        a4paper,% other options: a3paper, a5paper, etc
        left=4cm,
        right=4cm,
        top=3cm,
        bottom=3cm,
]
{geometry}

% Document Settings
\pagenumbering{arabic}
\title{Linear Statistical Model Assignment 2 -- 2019}
\author{Maleakhi Agung Wijaya, 784091}
\date{Tutor: Qiuyi, Tuesday 3:15 PM}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin Document
\begin{document}
\maketitle
\SweaveOpts{concordance=TRUE}

% Question One
\section{Question One}
I want to show that the maximum likelihood estimator of the error variance $\sigma^2$ is

\begin{equation*}
    \hat{\sigma} = \frac{SS_{Res}}{n}
\end{equation*}

% Proof
\noindent \textit{Proof:} \\
Assume $\epsilon \sim MVN(0, \sigma^2)$. Then we can calculate the likelihood function by multiplying density of the normal distribution as follows:

% Equation likelihood
\begin{eqnarray*}
    L(\beta, \sigma^2) &=& \prod_{i=1}^n \frac{1}{\sigma\sqrt{2\pi}} e^{-\epsilon_i^2 / 2\sigma^2} \\
    &=& \frac{1}{(2\pi\sigma^2)^{n/2}}e^{-\sum_{i=1}^n\epsilon_i^2/(2\sigma^2)} \\
    &=& \frac{1}{(2\pi\sigma^2)^{n/2}}e^{-(\textbf{y}-X\beta)^T(\textbf{y}-X\beta)/(2\sigma^2)}
\end{eqnarray*}

\noindent We maximise the likelihood by first taking log to create the log-likelihood function and then differentiating it with respect to $\sigma^2$ to find the Maximum Likelihood Estimator for $\sigma^2$:

% Equation log likelihood
\begin{eqnarray*}
    ln L(\beta, \sigma^2) = -\frac{n}{2} ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}(\textbf{y}-X\beta)^T(\textbf{y}-X\beta)
\end{eqnarray*}

% Equation differentiate log likelihood
\begin{eqnarray*}
    \frac{\partial ln L(\beta, \sigma^2)}{\partial \sigma^2} &=& -\frac{n}{2} \frac{1}{2\pi\sigma^2} 4\pi\sigma - \frac{SSRes}{2}\frac{-2}{\sigma^3} \\
    &=& -\frac{n}{\sigma} + \frac{SSRes}{\sigma^3} \\
    &=& 0
\end{eqnarray*}

\noindent Thus, solving the equation above, we have that
\begin{eqnarray*}
    &\rightarrow& \frac{SSRes}{\sigma^3} = \frac{n}{\sigma} \\
    &\therefore& \hat{\sigma^2} = \frac{SSRes}{n}
\end{eqnarray*}

\begin{flushright}
Q.E.D
\end{flushright}

% Question Two
\newpage
\section{Question Two}

\subsection{Part A}
From the question we have that the response variable is \textbf{Cars sold} with the other variables, such as \textbf{Cost}, \textbf{Unemployment rate}, and \textbf{Interest rate} as the predictor variables.

<<echo=TRUE>>=
# y
y <- c(5.5, 5.9, 6.5, 5.9, 8.0, 9.0, 10.0, 10.8)
print(y)

# X
X <- cbind(rep(1, length(y)), c(7.2, 10.0, 9.0, 5.5, 9.0, 9.8, 14.5, 8.0), 
           c(8.7, 9.4, 10, 9, 12, 11, 12, 13.7), 
           c(5.5, 4.4, 4.0, 7, 5, 6.2, 5.8, 3.9))
print(X)
@

\noindent I can calculate the estimate of parameters \textbf{b} by using $\textbf{b} = (X^TX)^{-1}X^T\textbf{y}$. As for variance, it can be computed using $\frac{SSRes}{n-p}.$

<<>>=
# b/ Parameter estimate
b <- solve(t(X) %*% X, t(X) %*% y)
print(b)

# Variance estimate
n <- length(y) # number of samples
p <- dim(X)[2] # number of parameters

s2 <- sum((y-X%*%b)^2) / (n-p) # SSRes / (n-p)
print(s2)
@

\noindent $\therefore$ Therefore, we have that \textbf{b} $= \begin{bmatrix}
    -7.4044796\\
    0.1207646\\
    1.1174846\\
    0.3861206
\end{bmatrix}$ and $SSRes = 0.3955368$

\subsection{Part B}
To calculate the covariance of the parameters \textbf{b}, we can inspect the covariance matrix of \textbf{b} which is given by $(X^TX)^{-1}\sigma^2$. However, recall that the question asked about parameters which have the highest covariance in their estimators. This means that we don't need to calculate $\sigma^2$ as it is always greater than 0 by the property of variance (and also since it is the square of $\sigma$). Therefore, we can calculate $(X^TX)^{-1}$ and inspect the off-diagonal column which have the highest magnitude.

<<>>=
# (XTX)-1
solve(t(X) %*% X)
@

\noindent $\therefore$ Therefore, we can see that the entries (1, 4) which correspond to ($\beta_0, \beta_3$) has the highest magnitude of covariance from the matrix given above.

\subsection {Part C}
The 99\% confidence interval can be calculated by using the derived formula $(x^*)^T\textbf{b} \pm t_\frac{\alpha}{2}s\sqrt{(x^*)^T(X^TX)^{-1}(x^*)}$ where $x^*$ is defined as $\begin{bmatrix}1 & 8 & 9 & 5 \end{bmatrix}^T$.

<<>>=
t <- c(1, 8, 9, 5)
t(t) %*% b + c(-1, 1) * qt(0.995, df=n-p) * sqrt(s2) * 
  sqrt(t %*% solve(t(X) %*% X) %*% t)
@

\noindent $\therefore$ Therefore, the confidence interval given by R is $(3.926075, 7.173129)$ or multiplied by thousands, we have the number of cars sold is $(3926.075, 7173.129)$.

\subsection{Part D}
I know from lecture that the prediction interval for the number of cars sold in a year is given by the derived formula of $(x^*)^T\textbf{b} \pm t_\frac{\alpha}{2}s\sqrt{1 + (x^*)^T(X^TX)^{-1}(x^*)}$. Given the interval (4012, 7087), I can calculate the significance level $\alpha$ by rearranging the formula to get $t_\frac{\alpha}{2}$ and later the confidence level used.

\begin{eqnarray*}
  (x^*)^T\textbf{b} + t_\frac{\alpha}{2}s\sqrt{1 + (x^*)^T(X^TX)^{-1}(x^*)} = 7.087 \\
  \rightarrow t_\frac{\alpha}{2} = \frac{7.087 - (x^*)^T\textbf{b}}{s\sqrt{1 + (x^*)^T(X^TX)^{-1}(x^*)}}
\end{eqnarray*}

\noindent Substituting the formula above with our data and calculating with R gives
<<>>=
t.alpha = (7.087 - (t %*% b)) / (sqrt(s2) * sqrt(1 + t %*% solve(t(X) %*% X) %*% t)) 

# Calculate the confidence level
pt(t.alpha, n-p)

# Thus the confidence level is 90% and we can also check using this
t(t) %*% b + c(-1, 1) * qt(0.95, df=n-p) * sqrt(s2) * 
  sqrt(1 + t %*% solve(t(X) %*% X) %*% t)
@

\noindent $\therefore$ Therefore, we know that the confidence level for the prediction interval is 90\%.

\subsection{Part E}
The model relevance test using a corrected sum of squares uses a hypothesis test in the form of $H_0: \gamma_1 = 0$ with $H_1: \gamma_1 \neq 0$ where $\gamma_1 = \begin{bmatrix} \beta_1 & \beta_2 & \beta_3 \end{bmatrix}^T$ and $\gamma_2 = \beta_0$. The test is given by the test statistics $\frac{R(\gamma_1 | \gamma_2)/(p-1)}{SSRes / (n-p)}$ which follow an F distribution with degree of freedom of $p-1$ and $n-p$. The procedure is given on R below:

<<>>=
H <- X %*% solve(t(X) %*% X) %*% t(X)
SSReg <- t(y) %*% H %*% y
SSRes <- sum((y-X%*%b)^2)

# By theorem, we have that Rg1g2 = SSReg - SSReg(reduced model)
Rg1g2 <- (SSReg - (sum(y))^2 / n) / (p-1)
denominator <- SSRes / (n-p)
Fstats <- Rg1g2 / denominator
pf(Fstats, p-1, n-p, lower=F) # one tailed test
@

\noindent $\therefore$ Therefore, the p-value that we have is 0.005317255 and we reject $H_0$ with 5\% significant level.

\section {Question Three}
\textbf{Claim:} $SSRes(Nested Model) \geq SSRes(Full Model)$ \\

\noindent \textit{Proof:} \\
Suppose we have two full rank models in which Model 1 is nested in Model 2. Assume for simplicity without loss of generality that Model 1 has 2 parameters $(\beta_0, \beta_1)$ and Model 2 has 3 parameters $(\beta_0, \beta_1, \beta_2)$. The idea of the proof stay the same for any number of parameters in Model 1 and Model 2 as long as Model 1 is completely nested in Model 2. \\ \\
\textbf{Model 1}: $y_i = b_0 + b_1 x_{i1} + e_i$ \\
\textbf{Model 2}: $y_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + e_i$ \\ \\
Since Model 1 is nested in Model 2, we know that the predictor variables for Model 1 is included in Model 2 along with some other predictor variable not in Model 1 $x_{i2}$. We now define the $SSRes$ as $\sum_{i=1}^n e_i^2 = (y - \widehat{E[y_i]})^2$ where $\widehat{E[y_i]} = b_0 + b_1 x_{i1} + ... +b_n x_{in}$.\\ \\
$\rightarrow SSRes($\textbf{Model 1}): $(y_i - b_0 - b_1 x_{i1})^2 $\\
$\rightarrow SSRes($\textbf{Model 2}): $(y_i - b_0 - b_1 x_{i1} - b_2 x_{i2})^2 $ \\ \\
Estimators for parameters in Model 1 can be obtained using the standard derived formula $(X^TX)^{-1}X^T\textbf{y}$. Now, by choosing the exact same value for $b_0, b_1$ and make $b_2 = 0$, we have obtained that $SSRes(Model1) = SSRes(Model2)$. For other values of $b_2$ (non-zero), it is always possible to optimise and therefore get a lower $SSRes$. This proof holds for general number of predictor variables as long as Model 1 is nested in Model 2.

\begin{flushright}
Q.E.D
\end{flushright}

% Question 4
\section{Question Four}
\subsection{Part A}
I will plot the linear model (without any transformation) with murder rate as response variable and other variables as predictor variables using \lstinline{pairs} command in R that are shown below. The model is given by $Murder = \beta_0 + \beta_1 Population + \beta_2 Income + \beta_3 Illiteracy + \beta_4 Life.Exp + \beta_5 HS.Grad + \beta_6 Frost + \beta_7 Area$.\\

<<fig=TRUE>>=
data(state)
statedata <- data.frame(state.x77, row.names=state.abb, check.names=TRUE)
pairs(statedata)
@

\noindent If we observe relationship between \lstinline{Murder} and other predictor variables, it can be seen that there are some evidence of heteroskesdasticity in \lstinline{Murder}, especially against \lstinline{population}, \lstinline{income}, \lstinline{HS.Grad}, and \lstinline{Area}. One solution of the problem is to consider taking logs of these variables and check it log transformation improved the fit. Apart from looking at the pairs plot, we can verify it using diagnostic plot 3 to search for signs of heteroskesdasticity which I also found in these variables that are mentioned above. Based on the observation, the new model is given by $Murder = \beta_0 + \beta_1 log(Population) + \beta_2 log(Income) + \beta_3 Illiteracy + \beta_4 Life.Exp + \beta_5 log(HS.Grad) + \beta_6 Frost + \beta_7 log(Area)$. The new pairs plot are shown below using R:

<<fig=TRUE>>=
statedata$Population <- log(statedata$Population)
statedata$Income <- log(statedata$Income)
statedata$HS.Grad <- log(statedata$HS.Grad)
statedata$Area <- log(statedata$Area)
pairs(statedata)
@

\subsection{Part B}
<<>>=
# Start with an empty model
basemodel <- lm(Murder ~ 1, data=statedata)
add1(basemodel, scope= ~ . + Population + Income + Illiteracy + Life.Exp + HS.Grad + 
       Frost + Area, test="F")
@

<<>>=
# Add Life.Exp as it has the smallest p value
model2 <- lm(Murder ~ Life.Exp, data=statedata)
add1(model2, scope= ~ . + Population + Income + Illiteracy + HS.Grad + 
       Frost + Area, test ="F")
@

<<>>=
# Add Frost as it has the smallest p value
model3 <- lm(Murder ~ Life.Exp + Frost, data=statedata)
add1(model3, scope= ~ . + Population + Income + Illiteracy + HS.Grad + 
       Area, test ="F")
@

<<>>=
# Add Area as it has the smallest p value
model4 <- lm(Murder ~ Life.Exp + Frost + Area, data=statedata)
add1(model4, scope= ~ . + Population + Income + Illiteracy + HS.Grad,
     test ="F")
@

\noindent Here we stop since none of the test is significant anymore. Therefore the parsimonious model using Forward Selection includes Life.Exp, Frost, and Area variables (necessary log transformation already applied).

\subsection{Part C}
<<>>=
#Start with a full model
model <- lm(Murder ~ Population + Income + Illiteracy + Life.Exp + 
              HS.Grad + Frost + Area, data=statedata)
step(model, scope= ~ . + Population + Income + Illiteracy + 
       Life.Exp + HS.Grad + Frost + Area)
@

\noindent Here we stop since AIC cannot get lower than 58.03. Using stepwise selection with AIC, we have that the final stepwise model only includes Population, Income, Illiteracy, Life.Exp, Frost, and Area as predictor variables.

\subsection{Part D}
The final fitted model is given by $Murder = \beta_0 + \beta_1log(Population) + \beta_2log(Income) + \beta_3Illiteracy + \beta_4Life.Exp + \beta_6Frost + \beta_7log(Area)$. The final model is calculated using stepwise selection with the AIC criterion as it is generally better than forward or backward selection.
<<>>=
final.model <- lm(Murder ~ Population + Income + Illiteracy + Life.Exp + Frost + 
     Area, data=statedata)
summary(final.model)
@

\subsection{Part E}
<<fig=TRUE>>=
# Plot (which = 1)
plot(final.model, which=1)
@

\noindent The plot here looks good enough since the residual mean is close to 0. Despite that we can see a trend in the plot, it is not enough to be a problem.
<<fig=TRUE>>=
# Plot (which = 2)
plot(final.model, which=2)
@

\noindent The plot looks good as the residual closely follows Normal Distribution.

<<fig=TRUE>>=
# Plot (which = 3)
plot(final.model, which=3)
@

\noindent The plot also looks good since no sign of heteroskesdasticity and small magnitude of residuals.

<<fig=TRUE>>=
# Plot (which = 5)
plot(final.model, which=5)
@

\noindent The plot looks good as no points with a really high Cook's Distance (< 0.5).

% Question 5
\section{Question Five}
\subsection{Part A}
We know that $\sum_{i=1}^ne_i^2 + \lambda\sum_{j=0}^kb_j^2 = \textbf{e}^T\textbf{e} + \lambda \textbf{b}^T\textbf{b}$. Then we can minimise the function with respect to \textbf{b} by taking partial derivative with respect to \textbf{b}.

\begin{align*}
  \rightarrow \frac{\partial (y-Xb)^T(y-Xb) + \lambda b^Tb}{\partial b} = 0 \\
  \rightarrow \frac{\partial[y^Ty - 2y^TXb + b^TX^TXb + \lambda b^Tb]}{\partial b} = 0 & \text{, since $y^TXb$ is a scalar (okay to transpose)} \\
  \rightarrow -2y^TX + 2X^TXb + 2\lambda b = 0 & \text{, since derivative $y^TAy = Ay + A^Ty$}\\
  \rightarrow (X^TX + \lambda I)b = X^Ty  \\
  \therefore b = (X^TX + \lambda I)^{-1}X^Ty
\end{align*}
\begin{flushright}
Q.E.D
\end{flushright}

\subsection{Part B}
The parameter estimate can be calculated by the formula given on part (a). It is calculated in R as follows:
<<>>=
y <- c(5.5, 5.9, 6.5, 5.9, 8, 9, 10, 10.8)
cost <- c(7.2, 10, 9, 5.5, 9, 9.8, 14.5, 8)
unemployment.rate <- c(8.7, 9.4, 10.0, 9.0, 12.0, 11.0, 12.0, 13.7)
interest.rate <- c(5.5, 4.4, 4, 7, 5, 6.2, 5.8, 3.9)
lambda <- 0.5
@

<<>>=
# Scale the response and predictor variables
cost <- scale(cost, center=TRUE, scale=TRUE)
unemployment.rate <- scale(unemployment.rate, center=TRUE, scale=TRUE)
interest.rate <- scale(interest.rate, center=TRUE, scale=TRUE)

y <- scale(y, center=TRUE, scale=FALSE)
X <- cbind(cost, unemployment.rate, interest.rate)
@

<<>>=
# Calculate parameter estimates
b <- solve(t(X) %*% X + lambda * diag(3)) %*% t(X) %*% y
print(b)
@

\noindent $\therefore$ Therefore, $\textbf{b} = \begin{bmatrix}0.3494789 \\ 1.7899861 \\ 0.3432961\end{bmatrix}$.

\subsection{Part C}
To calculate the optimal value for the penalty parameter $\lambda$, we can minimise the AIC function that is given on the assignment problem. We use R to iteratively find a good solution for $\lambda$ and plot $\lambda$ against AIC.

<<echo=TRUE, fig=TRUE>>=
lambda_S <- c()
AIC_S <- c()
n <- 8  # number of data
  
for (lambda in seq(0, 1, length=10)) {
  df <- sum(diag(X %*% solve(t(X) %*% X + lambda * diag(3)) %*% t(X)))
  b <- solve(t(X) %*% X + lambda * diag(3)) %*% t(X) %*% y
  SSRes <- sum((y - X %*% b)^2)
  AIC <- n * log(SSRes/n) + 2 * df
  lambda_S <- c(lambda_S, lambda) # put new lambda value
  AIC_S <- c(AIC_S, AIC) # put new AIC value
}

plot(AIC_S, lambda_S, main="Lambda vs AIC")
@

\noindent $\therefore$ From the plot, we can see that the lowest AIC which indicate the most optimal $\lambda$ is given by a value of $\lambda$ approximately 0.1.

\end{document} 
