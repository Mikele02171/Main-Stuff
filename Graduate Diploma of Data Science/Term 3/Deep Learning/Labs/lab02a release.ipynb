{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab02a release.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"5fsCX5JKBvKu"},"source":["# Lab 2a - Pytorch Basics\n","In today's lab we're going to actually get to build and train a complete neural network! The type of network we'll build is called a [Multilayer Perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron), which is fantastic for tabular data like what we'll be dealing with.\n","\n","The task is to predict the quality of wine based on features like alcohol content and density, using a [publically available dataset](https://archive.ics.uci.edu/ml/datasets/Wine+Quality). The target variable of \"quality\" is a subjective measure of the wine's quality based on expert tasters.\n","\n","We'll go through a number of steps:\n"," * Acquiring and exploring the dataset\n"," * Splitting the dataset into train / validation / test partitions\n"," * Implementing a neural network and optimiser\n"," * Writing a training loop\n"," * Visualising our results\n","\n","<font color='red'>In this notebook we will approach this as a regression problem where we try to predict the numerical value representing the quality rating (between 0 and 9). In the lab02b notebook we will approach this as a classification problem instead by making some simple modifications. </font>"]},{"cell_type":"markdown","metadata":{"id":"gZ7QEtgv9Amp"},"source":["## Data Acquisition\n","Let's acquire our dataset! To read and manipulate it, we'll be using [Pandas](https://pandas.pydata.org/), which is great for tabular data.\n","\n","Fortunately for us, Pandas is able to load a dataset when given either a local file path or a web URL. We'll take advantage of the latter, and directly download the dataset from the web."]},{"cell_type":"code","metadata":{"id":"W8AJW2PhBvNf"},"source":["# Import Pandas\n","import pandas as pd\n","\n","# There are two datasets available, but we'll just work with the larger, white\n","# wine dataset. Feel free to play around with the red wine dataset once you've\n","# finished the lab\n","red_wine_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n","white_wine_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n","\n","# It's a single function call to load a dataset. CSV files typically use commas\n","# as delimiters between records, but our dataset uses semicolons so we had to\n","# specify it with the \"delimiter\" argument.\n","all_data = pd.read_csv(white_wine_url, delimiter=';')\n","\n","print(type(all_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dgg3KyF0QXhg"},"source":["As we can see above, the type of object we just created is a `DataFrame`. This is how Pandas represent tabular data and provides a lot of functionality!\n","\n","Open [this link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) to the class documentation with a list of methods and their descriptions, and refer to it frequently during this lab."]},{"cell_type":"markdown","metadata":{"id":"Q7tbfD8Ytbtb"},"source":["## Data Exploration\n","The first step in any project is to get familiar with the dataset. It can be very tempting to jump ahead and start building a neural network, but a lot of unexpected problems can arise if you don't understand the dataset. This dataset is already very clean - meaning that there are few problems with it - so we don't need to work too hard at this."]},{"cell_type":"markdown","metadata":{"id":"v5PhMs-yQNNU"},"source":["First up, let's just have a look at a few rows of the dataset. This is achieved by calling the method `head` on your `DataFrame`. Calling this method will display a table of the first few example in the dataset."]},{"cell_type":"code","metadata":{"id":"j6TVSm0CPeGk"},"source":["all_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2frU8-Rmy0pu"},"source":["As we can see, our dataset has a number of columns including density, pH and alcohol contents. \\\n","Our task today is to predict the last column - **quality**. This is a 0-10 value indicating the wine's quality score, as measured by wine tasting experts.\n","A score of 0 is terrible wine, and a score of 10 is the best wine possible."]},{"cell_type":"markdown","metadata":{"id":"FScJRVVB9oA3"},"source":["### Check for Missing Data\n","It's very important that we check for any missing data before using it. Passing a missing value into a neural network will result in unexpected behaviour, and almost certainly degrade its performance. To avoid this we first check if there are any null values, and fix those that we encounter.\n","\n","*This dataset doesn't contain any null values, so we don't need to fix it. Common strategies include replacing nulls with the mean value for that attribute, the value zero, or just deleting the entire example.*"]},{"cell_type":"code","metadata":{"id":"-INHVBg79sN9"},"source":["# Set each value in the DataFrame to True where it's null, then count the number\n","# of True values\n","all_data.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRlwDZywtmFw"},"source":["### Plot Feature Distributions\n","Now that we're confident that there is no missing data, we can look at the distribution of the features. We'll do this using a histogram, which is a type of bar graph which counts the number of values that fall into different regions.\n","\n","Luckily for us, `DataFrame` objects have a method called `hist` which does the hard work for us!"]},{"cell_type":"code","metadata":{"id":"MuJD-jKyvpwt"},"source":["all_data.hist(bins=20, figsize=(15, 10));"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_RmShl6L9xy3"},"source":["Looking at the distribution of the target attribute `quality`, we can see that there are very few examples with values of 3, 4, 8, and 9. This means that the dataset is unbalanced, usually resulting a model that performs poorly on the underrepresented examples. We would normally apply [a few tricks](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) to resolve this, but we will just ignore it for this lab."]},{"cell_type":"markdown","metadata":{"id":"fZncp6399yUU"},"source":["## Partition the Dataset\n","Now we'll split our dataset into three partitions - training, validation and testing. Each of these partitions have a specific function in guiding the experimental process:\n"," * Training: the partition which the model directly uses during weight optimisation during training.\n"," * Validation: the model isn't allowed to use this partition during training; we instead fine-tune our hyperparameters (e.g. initial learning rate, optimization algorithm used, etc.) based on the model's performance on this sample.\n"," * Testing: this partition is used to evaluate model performance on previously unseen data. We shouldn't use this partition to optimise the model *or* fine-tune hyperparameters. Usually you only use this partition once at the very end when you want to see just how well your trained model generalizes to completely new data."]},{"cell_type":"markdown","metadata":{"id":"y1cAvwonXsMD"},"source":["As our task is to predict the wine quality from the other attributes, we will first have to separate the wine quality column from the rest. We typically call the input features `x` and the target features `y`.\n","\n","Before doing so, let's take a look at how to handle columns in Pandas.\n","\n"]},{"cell_type":"code","metadata":{"id":"ybsXbCl5YDfJ"},"source":["# We use the following function to extract a single column from a DataFrame. \n","# The function returns a series instead of a DataFrame. \n","# A series is essentially a 1D array, so it doesn't have any concept of columns.\n","print(all_data[\"sulphates\"])\n","print('-' * 50)\n","\n","# Here we end up with the same data as above, but this time we receive a DataFrame\n","# with a single column (note how we passed the column name in a list) instead of series.\n","print(all_data[[\"sulphates\"]])\n","print('-' * 50)\n","\n","# The code below returns multiple columns\n","print(all_data[[\"sulphates\", \"density\"]])\n","print('-' * 50)\n","\n","# The code below returns all *but* one column. We need to pass axis=1 to indicate\n","# that we're dropping a column, not a row.\n","print(all_data.drop(\"sulphates\", axis=1))\n","print('-' * 50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HrvUipMNt06C"},"source":["<font color='red'>In the next cell, separate the inputs and labels into two variables, using the cell above for reference.</font>"]},{"cell_type":"code","metadata":{"id":"mcEDRSKjrI18"},"source":["target_column = \"quality\"\n","\n","# TODO: Extract just the *input* features. Instead of specifying all of the features we\n","# want, we should drop the feature we *don't* want.\n","# x_data = ...\n","\n","\n","# TODO: Extract the target feature. We don't want a Series here, but a DataFrame\n","# with one column (see the above cell)\n","# y_data = ...\n","\n","\n","# If your implementation is correct the shape of y_data should be (4898, 1)\n","# So y_data is a 2D tensor containing 4898 examples and just 1 feature.\n","print(\"y_data shape:\", y_data.shape) \n","\n","# Just like with tensors, we can print the shape\n","num_examples = x_data.shape[0]\n","num_input_features = x_data.shape[1]\n","\n","# If your implementation is correct the number of samples should be 4898\n","print(\"Number of examples:\", num_examples) \n","\n","# If your implementation is correct the number of input features should be 11\n","print(\"Number of input features:\", num_input_features)\n","\n","# If your implementation the shape of the x_data tensor should be (4898, 11)\n","# which means it is a 2D array where each row represents one example and each\n","# column represents one feature\n","print(\"x_data shape:\", x_data.shape )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ogYWdNvabYP4"},"source":["We've split the dataset into inputs and targets, but now we need to actually partition it into train, val, and test splits. There are numerous ways of doing this, but here we'll present the most simple."]},{"cell_type":"markdown","metadata":{"id":"zZwPbJpHej5S"},"source":["First up, we'll convert our `DataFrame` objects into tensors so we can have torch perform the random partitioning.\n","\n","<font color='red'>In the next cell, complete the code to convert the data to PyTorch tensors.</font>"]},{"cell_type":"code","metadata":{"id":"R4dK-21abmtk"},"source":["# Import Torch and the dataset utilities we need\n","import torch\n","from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n","\n","# The percentages for each partition\n","TRAIN_SPLIT = 0.8\n","VAL_SPLIT = 0.1\n","TEST_SPLIT = 0.1\n","# Ensure that the splits add to 100%\n","assert TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT == 1\n","\n","\n","# TODO: Create two tensors and initialise them with x_data.values and y_data.values.\n","# The dtype should be torch.float32. DataFrame.values directly returns the 2D\n","# data in the dataframe, which is what Torch requires to initialise a tensor.\n","# we have done the initialization for x_tensor for you. Please do something similar\n","# the y_tensor.\n","\n","x_tensor = torch.tensor(x_data.values, dtype=torch.float32)\n","# y_tensor = ...\n","\n","\n","assert torch.is_tensor(x_tensor) and x_tensor.dtype == torch.float32\n","assert torch.is_tensor(y_tensor) and y_tensor.dtype == torch.float32\n","\n","# Now we construct a TensorDataset - a simple class used to associate each x and\n","# y value in our tensors.\n","full_dataset = TensorDataset(x_tensor, y_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R199K4j-e2HM"},"source":["Now we just need to compute the number of examples in each partition and actually split the dataset into train, val, and test.\n","\n","<font color='red'>In the next cell, complete the code to split the data into three partitions.</font>"]},{"cell_type":"code","metadata":{"id":"wX2E5L1Fx3p3"},"source":["# Calculate the number of examples in each partition\n","train_size = int(TRAIN_SPLIT * len(all_data))\n","val_size = int(VAL_SPLIT * len(all_data))\n","test_size = len(all_data) - train_size - val_size\n","\n","print(\"Train examples:     \", train_size)\n","print(\"Validation examples:\", val_size)\n","print(\"Test examples:      \", test_size)\n","\n","# Before we actually split the dataset, we seed Torch's random number generator.\n","# This ensure that we end up with the exact same partitions every time it's run.\n","torch.manual_seed(42)\n","\n","# TODO: Split the dataset using the random_split function we imported earlier.\n","# The function takes a dataset and a list of partition lengths.\n","# Hint: We already have all of these variables available\n","#       You can see an example here:\n","#       https://www.programcreek.com/python/example/125046/torch.utils.data.random_split\n","# train_dataset, val_dataset, test_dataset = random_split(...)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-4h0HJyxCcIf"},"source":["## Number of data items in each dataset\n","\n","A simple way to find out the number of data items in a dataset is to use the len function."]},{"cell_type":"code","metadata":{"id":"ZB86JThhCosm"},"source":["print(\"train set size: \", len(train_dataset))\n","print(\"validation set size: \", len(val_dataset))\n","print(\"test set size: \", len(test_dataset))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-XI39wwe-i6"},"source":["## Create the DataLoaders\n","Torch uses `DataLoader` objects during training to sample values from each `Dataset`. The `DataLoader` handles shuffling the data, as well as gathering the examples into batches. We will need three loaders, as we have three partitions.\n","\n","<font color='red'>In the next cell, initialise three `DataLoader` objects - one for each of the partitions.</font>"]},{"cell_type":"code","metadata":{"id":"z3NZrKhIeTvB"},"source":["# When you've finished the lab, try modifying the batch size to see what effect\n","# it has on your results\n","BATCH_SIZE = 64\n","\n","# TODO: Construct a DataLoader for each Dataset. The constructor takes three\n","# arguments - a Dataset, the batch size, and a boolean indicating whether it\n","# should shuffled. We will set shuffle=True for train dataloader and \n","# shuffle=False for the other two. We have written the train_loader for you\n","# you should write the rest.\n","\n","train_loader = train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","# val_loader = DataLoader(...\n","# test_loader = DataLoader(...\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D2uPyQrl0Qkh"},"source":["We now have three DataLoaders, but how do they work?\n","\n","In the previous lab we saw that we can iterate over a list of values very easily in Python:\n","```python\n","for x in [1, 2, 3]:\n","    # Do something with x\n","```\n","\n","The `DataLoader` class gives us that same functionality but with a batch of inputs and labels at a time instead of a single value:\n","```python\n","for batch in my_dataloader:\n","    inputs, labels = batch  # <- tuple unpacking*\n","    # Do something with the inputs and labels\n","```\n","*_You might not have seen tuple unpacking before. This Python feature allows you to extract values from a list or tuple. For example, this code would give x and y the values of 1 and 2, respectively: `x, y = (1, 2)`_\n","\\\n","\\\n","<font color='red'>In the next cell, iterate over the training `DataLoader` and insert code to do the following</font>:\n"," 1. Unpack the inputs and labels for the batch\n"," 1. Print out their shapes\n"," 1. Exit the loop early by using a `break` statement. (so we don't loop over the entire dataset)\n","\n"," The output should be for the input and label shapes:\n"," \\\n"," torch.Size([64, 11]) torch.Size([64, 1])\n"]},{"cell_type":"code","metadata":{"id":"-Ooi-PKq0Ny8"},"source":["# TODO: Loop over the training dataloader\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v59BnxUq3TqC"},"source":["Take note of the shapes:\n"," * The input shape is `[batch_size, num_input_features]`\n"," * The output shape is `[batch_size, num_target_features]`"]},{"cell_type":"markdown","metadata":{"id":"2ldsYbNK96_t"},"source":["## Set Up for Training\n","We're finally done with data manipulation! Data manipulation is typically the most time-intensive and important task in a deep-learning project, so consider yourself lucky!\n","\n","Now we'll go through everything from building the model, to writing a complete training loop!"]},{"cell_type":"markdown","metadata":{"id":"ojUURBDh9-hm"},"source":["### Enable GPU Training *(if available)*\n","The rise in popularity of deep learning is largely a result of the availability of good Graphics Processing Units. So although it's not required, it's definitely good to utilise a GPU if you can.\n","\n","It's exceptionally easy to use a compatible GPU in Pytorch - we can do it in just a few lines of code!"]},{"cell_type":"code","metadata":{"id":"2co2qquU7lcR"},"source":["# By default we'll assume that GPU acceleration isn't available\n","device = torch.device(\"cpu\")\n","\n","# Check if GPU acceleration is available (requires a CUDA-compatible GPU) and\n","# set the device variable accordingly. If the computer has more than one GPU,\n","# you can specify which one by replacing 0 with a different index\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","    torch.cuda.set_device(device)\n","\n","print(\"Training on\", device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRbspMJc-Egx"},"source":["### Define the Model\n","Torch allows us to build very complex custom models. For today's task we don't need anything complicated - we'll save that for later labs ðŸ˜‰\n","\n","To implement a model, we need to write a subclass of `nn.Module`. This class should implement a `forward` function, which is called with a batch of inputs to make predicitons.\n","\n","Our model will be composed of two linear layers, with an activation function inbetween. The easiest way to do this is to pass a `nn.Sequential` object with the layers as arguments. Torch stitches them together, so inputs are passed through each of the layers sequentially.\n","\n","<font color='red'>In the next cell, initialise a `Sequential` module with the specified layers.</font> \\\n","Open up the links below for documentation and examples:\n"," * [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n"," * [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n"," * [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)"]},{"cell_type":"code","metadata":{"id":"SM8YqMMz0kCE"},"source":["# Import the neural network module of Pytorch. We access its methods like \"nn.Linear\"\n","import torch.nn as nn\n","\n","# Our model class must subclass nn.Module\n","class MLP(nn.Module):\n","    # The __init__ method is similar to a constructor like you find in other\n","    # languages. We will take the device as an argument to transfer the model to the GPU\n","    def __init__(self, device):\n","        super().__init__()\n","        # TODO: Initialise a Sequential module consisting of the below layers, and\n","        # store it in the member variable self.seq\n","        #  - a Linear layer mapping from num_input_features to 20 hidden features\n","        #  - a ReLU activation layer\n","        #  - a Linear layer mapping from 20 hidden features to a single feature (the wine quality)\n","        # You can look here for an example:\n","        #     https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n","        # self.seq = nn.Sequential(...\n","\n","        \n","            \n","        # The model stays on the CPU by default. Calling the \"to\" method transfers\n","        # the model weights to whichever device we specified\n","        self.to(device)\n","\n","    # Our forward method simply takes the input batch x, passes it through our\n","    # Sequential module, and returns the outputs (predictions)\n","    def forward(self, x):\n","        return self.seq(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JVlR_DGsuITZ"},"source":["## Simple Training loop\n","\n","In the cell below we will write a very simple training loop to train the above model. This will allow you to see the whole training process at once. However, in proper production systems programmers write code in a more modular fashion. Maximizing the amount of reuse of code between, train and test loops. In the rest of the lab we will show you the more modular way to structure your code, which is closer to what is actually used in industry."]},{"cell_type":"code","metadata":{"id":"MjR6AGaWvV85"},"source":["import torch.optim as optim\n","\n","# Use the mean squared error as the loss function\n","criterion = nn.MSELoss()\n","# Use MLP as the model\n","model = MLP(device)\n","# Use the SGD optimizer with initial learning rate set to 0.0001\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n","# The number of times we loop over the entire dataset\n","total_epochs = 10\n","for epoch in range(total_epochs):  # loop over the dataset multiple times\n","\n","    # The following is computed in a single pass through the dataset\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # Copy the data to the specified device\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Put the model into train mode\n","        model.train()\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        # Compute the loss using the loss function\n","        loss = criterion(outputs, labels)\n","\n","        # Perform backprop using the loss\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","     # print statistics\n","    print('epoch: %d loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n","        \n","print('Finished Training')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W_4VO_ZI6h8c"},"source":["## Simple Testing loop\n"]},{"cell_type":"code","metadata":{"id":"crJwn4Z56iTE"},"source":["running_loss = 0.0\n","for i, data in enumerate(test_loader, 0):\n","   # get the inputs; data is a list of [inputs, labels]\n","   inputs, labels = data\n","\n","   # Copy the data to the specified device\n","   inputs, labels = inputs.to(device), labels.to(device)\n","\n","   # Put the model into eval mode\n","   model.eval()\n","\n","   with torch.no_grad():\n","    # Forward + backward + optimize\n","    outputs = model(inputs)\n","\n","    # Compute the loss using the loss function\n","    loss = criterion(outputs, labels)\n","    running_loss += loss.item()\n","\n","print(\"test loss: \", running_loss/len(test_loader))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fiki5n2rvebh"},"source":["# Modular training and test code follows\n","\n","The following code will decompose the training loop into functions which maximizes the reuse of code within this project and also allows the code to be reused across other projects."]},{"cell_type":"markdown","metadata":{"id":"qgPp5Zs6-JqB"},"source":["### Set Up the Loss Function\n","In order to train the network, we need a loss function! A typical loss function for a regression task like this is [mean squared error (MSE)](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html).\n","\n","We'll try different loss functions later, but this will do for now.\n","\n","<font color='red'>In the next cell, initialise the MSE loss function</font>. \\\n","Refer to the above link if necessary."]},{"cell_type":"code","metadata":{"id":"GAU0jpBW7ysM"},"source":["# Instantiate the MSE loss function with no arguments\n","# loss_func = ...\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2LeU3_4b-Ouw"},"source":["### Set Up the Optimiser\n","An optimiser is used to update the model weights, and there are many variants available. For now we'll use stochastic gradient descent (SGD), which is the simplest optimiser. We'll try some variants later on and see how it effects our results."]},{"cell_type":"code","metadata":{"id":"XCUXpc3k2b_y"},"source":["# We'll construct our optimiser inside a function - some optimisers have variables\n","# and we want to make sure they start fresh each time we train a model.\n","def construct_optimizer(model):\n","    # When constructing an optimiser, must we provide the model parameters we\n","    # with to optimise\n","    return torch.optim.SGD(model.parameters(), lr=0.0001)\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BRc1VWJG-YCX"},"source":["### Implement the Training Loop\n","Training requires quite a few things to happen, so we'll break it up into different functions to make it easier."]},{"cell_type":"markdown","metadata":{"id":"Ku1JOZ8kGdSM"},"source":["First we'll write a function which performs a single training step on a batch of inputs/targets. In this function, we'll:\n"," 1. Copy the data to the GPU *(if available)*\n"," 2. Put the model into `train` mode *(some layers operate differently at train and test time)*\n"," 3. Set all parameter gradients to zero *(to start fresh in every training step)*\n"," 4. Have the model make a prediction for the batch\n"," 5. Compute the loss *(between predictions and targets)*\n"," 6. Backpropagate the loss to compute gradients\n"," 7. Update model weights\n","\n","So we can visualise the training procedure, we'll return the loss for the batch. We'll also return the predictions, as they may come in handy.\n","\n","As there are a lot of steps, this has been implemented for you. Read the code and comments thoroughly, as it's vital to understand the training procedure."]},{"cell_type":"code","metadata":{"id":"8pNvnds8F11a"},"source":["def train_step(model, inputs, labels, loss_func, optimizer, device):\n","    # 1. Copy the data to the specified device\n","    inputs, labels = inputs.to(device), labels.to(device)\n","\n","    # 2. Put the model into train mode\n","    model.train()\n","    \n","    # 3. Set all parameter gradients to zero\n","    optimizer.zero_grad()\n","\n","    # 4. Have the model make a prediction for the batch\n","    outputs = model(inputs)\n","\n","    # 5. Compute the loss\n","    loss = loss_func(outputs, labels)\n","\n","    # 6. Backpropagate the loss to compute gradients\n","    loss.backward()\n","    \n","    # 7. Update model weights\n","    optimizer.step()\n","\n","    # Return the loss and outputs. As loss is a tensor with a single value, we\n","    # use .item() to extract this as a regular float value\n","    return loss.item(), outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJ8S44lwIwg7"},"source":["Let's now write a function which performs a single *test* step on a batch of inputs/targets. We won't write a different function for validation and test, as they both perform the same task, just on different data splits. This has been left as an exercise for you, as it's very similar to `train_step`. A lot of the lines are either the same or the same but with some small modifications. So  understanding `train_step` will really help you do this block.\n","\n","<font color='red'>In the next cell, finish the implementation of `test_step`.</font> \\\n","This function should:\n"," 1. Copy the data to the GPU *(if available)* Same as line 1 of `train_step`\n"," 2. Put the model into `eval` mode *(some layers operate differently at train and test time)* similar to what we did for line 2 of `train_step` but with eval instead.\n"," 3. Have the model make a prediction for the batch\n"," 4. Compute the loss *(between predictions and targets)*\n"," 5. Steps 3 and 4 should be placed inside a `with torch.no_grad():` block to prevent gradients being calculated. Since we are just doing evaluation so there is no need to perform back propagation. See [the docs](https://pytorch.org/docs/stable/generated/torch.no_grad.html) for an example.\n","\n","Like with `train_step`, we'll return the loss and outputs for later."]},{"cell_type":"code","metadata":{"id":"dadTeVe7F4cb"},"source":["def test_step(model, inputs, labels, loss_func, device):\n","    # 1. Copy the data to the specified device\n","    # SOLUTION LINE\n","    inputs, labels = inputs.to(device), labels.to(device)\n","\n","    # 2. Put the model into eval mode\n","    # SOLUTION LINE\n","    model.eval()\n","\n","    # 5. Place steps 3 and 4 into a torch.no_grad block\n","    with torch.no_grad():\n","\n","    return loss.item(), outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aYL2FYI-L5x-"},"source":["Now it's time for the actual training loop, which has been implemented for you. \n","\n","It might look like a lot of code, but it's actually mostly comments. As before, take your time and read through this section to ensure you understand what's happening.\n","\n","In this function, the model is trained for `num_epochs`, where the validation loss is computed at the end of each epoch. The training and validation losses are returned as a list at the end so we can plot the loss curves."]},{"cell_type":"code","metadata":{"id":"r5Fwe9oiF8D1"},"source":["# We'll use this library to print a nice progress bar during training\n","from tqdm.notebook import tqdm\n","\n","def train(model, train_loader, val_loader, num_epochs, loss_func, optimizer, device):\n","    # A couple of lists to record losses at the end of each epoch\n","    train_losses = []\n","    val_losses = []\n","\n","    # A loop for each of the epochs, using the fancy tqdm progress bar\n","    for epoch in tqdm(range(num_epochs)):\n","        # We'll add up the training and validation losses for this epoch\n","        train_loss = 0\n","        val_loss = 0\n","\n","        # Thanks to the work we did writing our train and test step functions,\n","        # training on the entire dataset is pretty easy!\n","        for batch in train_loader:\n","            # Here we just unpack the batch into inputs and labels, then call\n","            # our train_step function. As we don't want the predicted values, we\n","            # use an underscore for the second return value\n","            inputs, labels = batch\n","            loss, _ = train_step(model, inputs, labels, loss_func, optimizer, device)\n","            train_loss += loss\n","\n","        # Compute loss for the entire validation set\n","        for batch in val_loader:\n","            inputs, labels = batch\n","            loss, _ = test_step(model, inputs, labels, loss_func, device)\n","            val_loss += loss\n","\n","        # At the end of the epoch, compute the mean loss for this epoch and add\n","        # it to the lists\n","        train_losses.append(train_loss / len(train_loader))\n","        val_losses.append(val_loss / len(val_loader))\n","            \n","    # At the end of training, return our lists of loss\n","    return train_losses, val_losses"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XXqcF9OI5Icy"},"source":["Similar to the training loop function above, we write a function to run our model on the test dataset. This is similar to the training loop, but with a couple of small differences:\n"," * It is run for only one epoch, as we just want to compute the loss for the entirety of the test split\n"," * The targets and predictions are returned for us to visualise"]},{"cell_type":"code","metadata":{"id":"zl7U4usA2sDs"},"source":["# Whereas PyTorch excels at GPU machine learning, Numpy is a great general-purpose\n","# numerical computing package with more features. So for some of our miscellaneous\n","# number-crunching, we'll use Numpy instead. The Numpy equivalent of the torch\n","# tensor is the numpy array.\n","import numpy as np\n","\n","\n","def test(model, test_loader, loss_func, device):\n","    # We'll store the targets, predictions and losses in lists to return\n","    targets = []\n","    predicted = []\n","    losses = []\n","\n","    for batch in test_loader:\n","        # Just like when training, unpack the batch and make predictions\n","        inputs, labels = batch\n","        loss, outputs = test_step(model, inputs, labels, loss_func, device)\n","\n","        losses.append(loss)\n","        # The outputs and labels tensors are tensors that may be on the GPU, so\n","        # here we detach them from the graph so PyTorch doesn't compute their\n","        # gradients, then transfer them to the CPU and convert to numpy arrays\n","        predicted.extend(outputs.detach().cpu().numpy())\n","        targets.extend(labels.detach().cpu().numpy())\n","    \n","    # Just return the mean loss, as we'll use this for our final score\n","    return np.mean(losses), targets, predicted\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eK8ZLj5J-hc6"},"source":["## Train the Model\n","All of the hard work is done! Now we just need to call the functions we defined above, and let the model train.\n","\n","As with performing the data partitioning, we'll seed Torch's random number generator so that our results are reproducible.\n","\n","It may take a little while to train the model, so be patient. If you just want to check that all of the code appears to be working, you can reduce the number of epochs from 100 to something smaller. *Don't forget to put it back again!*\n","\n","<font color='red'>In the next cell, make sure you know the purpose of every parameter being passed to `train`. If you are unsure, look at the function signature, and where the respective variables are defined.</font>"]},{"cell_type":"code","metadata":{"id":"yKHAozL-2JZb"},"source":["# Seed Torch's random number generator like in a previous cell\n","\n","torch.manual_seed(42)\n","\n","model = MLP(device)\n","\n","optimizer = construct_optimizer(model)\n","\n","# Train the model for 100 epochs and store the training and validation losses.\n","train_loss, val_loss = train(model, train_loader, val_loader, 100, loss_func, optimizer, device)\n","\n","# Test the model and store the test loss, targets, and predictions.\n","test_loss, test_targets, test_predictions = test(model, test_loader, loss_func, device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yk9kgPiP-l4k"},"source":["### Visualise the Results\n","Great! The model has been trained, but we have no idea how well it performed! Luckily, we returned the training and validation loss for each epoch. Let's plot that to a graph and see what we can learn from it.\n","\n","\n","With this graph, we also print a couple of metrics:\n"," * [Root-mean-squared-error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) - what the average difference between our predictions and the targets is. I.e. when our model predicts a wine's quality, how close it is to the true quality score on average. The best possible score is 0.\n"," * [RÂ²](https://en.wikipedia.org/wiki/Coefficient_of_determination) - this is a measure of how well predictions approximate the ground truth. The good thing about this metric is that it is out of 1, so the best possible score is 1 - anything above 0.7 is usually considered a good result.\n"]},{"cell_type":"code","source":["# This installs the torchmetrics library\n","!pip install torchmetrics"],"metadata":{"id":"NSVrdycwIxdq"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0lvQL0cMxTzX"},"source":["# Import a graph-plotting library\n","import matplotlib.pyplot as plt\n","\n","# The next few lines plot our loss values for each epoch to a scatter-plot\n","epochs = range(len(train_loss))\n","plt.figure(figsize=(15, 6))\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(epochs, train_loss, 'r-', label='Training')\n","plt.plot(epochs, val_loss, 'b-', label='Validation')\n","plt.legend()\n","plt.show()\n","\n","#from sklearn.metrics import mean_squared_error, r2_score\n","from torchmetrics import MeanSquaredError, R2Score\n","\n","mean_squared_error = MeanSquaredError(squared=False)  #squared (bool) â€“ If True returns MSE value, if False returns RMSE value.\n","print(\"RMSE: \", \"{:.3f}\".format(mean_squared_error(torch.as_tensor(test_predictions), torch.as_tensor(test_targets))))\n","\n","r2_score = R2Score()\n","print(\"RÂ²: \", \"{:.3f}\".format(r2_score(torch.as_tensor(test_predictions),torch.as_tensor(test_targets))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kv2-_Bqc6lZ6"},"source":["#### Interpreting the Loss Curves\n","\n","Loss curves can tell us a lot about training and take a bit of practice to interpret, but here's a quick overview of what they typically mean.\n","\n","**The loss bounces around wildy** \\\n","The learning rate is far too high and the model isn't able to settle upon a set of parameters.\n","\n","**The loss goes down sharply then quickly flattens out** \\\n","The learning rate is a bit too high, so is likely not finding the best set of parameters.\n","\n","**The loss doesn't stop going down** \\\n","Either the learning rate is too low, or the number of epochs (number of passes throught the entire dataset) isn't large enough. 100 epochs should be enough for this dataset, so it's likely that the learning rate is too low.\n","\n","**Both losses go down, the validation loss starts going up again** \\\n","The model is overfitting to the training dataset. This indicates that the model is being trained for too many epochs, or it could indicate a deeper problem with the model/dataset.\n","\n","**The ideal loss curve** \\\n","Should have a fairly rapid descent, then slowly smooth out to a near horizontal line.\n","\n","*Although this is what these typically mean, it may not necessarily be true!*"]},{"cell_type":"markdown","metadata":{"id":"NRzhDMdn9Ut6"},"source":["#### Plotting the Predictions and Targets\n","\n","Now we've seen the model loss, we'll take a look at how our predictions compare to the targets.\n","\n","A common way for regression tasks like this one is to plot the targets and predictions on a scatter plot. This shows us for each target value on the x-axis, what our model predicted on the y-axis.\n","\n","The graph for a perfectly performing model will look like a staight line from the bottom-left to the top-right, indicating that every predicted value matched the target value exactly - though this is practically impossible."]},{"cell_type":"code","metadata":{"id":"HvNe2a3yExOt"},"source":["plt.figure(figsize=(8, 8))\n","plt.xlabel(\"Target value\")\n","plt.ylabel(\"Predicted value\")\n","plt.xlim((0, 10))\n","plt.ylim((0, 10))\n","plt.xticks([i for i in range(0, 11)])\n","plt.yticks([i for i in range(0, 11)])\n","plt.plot(test_targets, test_predictions, 'r.')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wT9neu69_sgo"},"source":["## Improve Performance\n","Now is the time to see how well you can get the model to perform! How high of an RÂ² score can you get? By tweaking the model, optimiser and loss function it's possible to achieve an RÂ² value of over `0.33`. The optimal value for RÂ² is 1 so even 0.33 is not very good. Usually you would want a value of at least 0.7 to say the predicted and ground truth values are correlated. This is maybe due to the fact we made the problem a regression problem, and is better modeled as a classification problem instead. In the lab02b notebook we will try to turn this problem into a classification problem and see what happens."]},{"cell_type":"markdown","metadata":{"id":"IAc35TPd4joF"},"source":["### The Model\n","The model architecture is an obvious choice for improving our results. The current model is a 2-layer MLP with a hidden size of 20.\n","\n","<font color='red'>Improve performance by modifying the model in the the \"Define the Model\" section</font>\n","\n","Here are a few ideas you can try to improve performance:\n"," * Change the hidden size\n"," * Change the activation function\n","  * [Check out the PyTorch docs](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) for a list of options. Common choices are ReLU, ELU, LeakyReLU, Sigmoid and Tanh.\n"," * Make the model deeper with more layers"]},{"cell_type":"markdown","metadata":{"id":"enW0mpuAAtVc"},"source":["### The Loss Function\n","The loss function we used is standard for regression tasks, but it has a bit of a problem. It is very sensitive to outliers, which means that we unfairly punish the model for making bad predictions on values which don't really fit into the dataset in the first place!\n","\n","Try the below loss functions and see which works best.\n","\n","<font color='red'>Improve performance by modifying the loss function in the the \"Set Up the Loss Function\" section</font>\n","\n","```python\n","# L1 loss handles outliers better, but usually doesn't perform as well on the rest of the dataset\n","loss_func = nn.L1Loss()\n","# Smooth L1 Loss (aka Huber loss) is a combination of L1 and MSE loss, with the benefits of both\n","loss_func = nn.SmoothL1Loss()\n","```"]},{"cell_type":"markdown","metadata":{"id":"43C-in11AM3j"},"source":["### The Optimiser\n","So far we're only using the stochastic gradient descent (SGD) optimizer, which is the simplest type of optimiser. Try one of the below optimisers and play around with different parameters learning rates and momentum.\n","\n","<font color='red'>Improve performance by modifying the optimiser in the the \"Set Up the Optimiser\" section</font>\n","\n","```python\n","# Add momentum to the model to make training more stable\n","torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.95)\n","# Try a more complex optimiser like Adam (this is usually recommended as the default optimiser). Adam is a good general purpose optimizer since it will automatically adjust the learning during training, so it make tunning the optimizer easier.\n","torch.optim.Adam(model.parameters(), lr=0.001)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vHqXP4oFBUir"},"source":["\n","## Classification problem instead\n","\n","Lets try to solve this problem again but as a classification problem instead. Head over to lab02b notebook for us to have a crack at that."]}]}