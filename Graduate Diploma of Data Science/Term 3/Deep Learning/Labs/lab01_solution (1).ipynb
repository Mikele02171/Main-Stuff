{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab01 solution.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"5fsCX5JKBvKu"},"source":["# Lab 1 - Introduction to Python and Pytorch\n","In this lab, we'll get familiar with the Python programming language and the Pytorch machine learning framework. The combination of Python and Pytorch facilitate rapid machine learning development and experimentation, while also being suitable for production-ready systems. We're going to be using Pytorch for most of this subject, so it's imperative that you invest some time learning how to best utilise it.\n","\n","By the end of this lab, you will:\n"," - understand Python syntax and control structures\n"," - be familiar with Pytorch tensor operations\n","\n","<font color='red'>Note don't get stress if you do not finish the labs in 2 hours. The labs are designed to give you comprehensive knowledge in a particular area. Sometimes that does not fit exactly into 2 hours. Please continue to work on the labs in your own time to get the most out of them. Like maths subjects were students are given homework to complete themselves.</font>"]},{"cell_type":"markdown","metadata":{"id":"iSsEeTrTBvKv"},"source":["## Google Colab and Jupyter Notebooks\n","Most of the coding in this subject is done in Jupyter Notebooks. Each Notebook is made up of cells containing either Markdown formatted text (like this cell) or Python code (like those below). You can run a cell by pressing `<SHIFT>+<ENTER>` or the \"Run\" button at the top of the window.\n","\n","There are two ways you might access this Jupyter Notebook for this subject: host it yourself, or use Google Colab to host a free virtual machine to host it for you.\n","\n","#### Code cells\n","Code cells are edited like a typical text-editor, and running one will execute the Python code it contains, with any outputs displayed below it.\n","\n","#### Markdown cells\n","Markdown cells contain Markdown formatted text (like this cell). To edit the contents, just double click on it. Running a Markdown cell applies the formatting."]},{"cell_type":"markdown","metadata":{"id":"8xA16MBFBvKw"},"source":["## Python practice\n","It is assumed that you have some programming experience, so we'll just explain how Python may differ from other languages you've seen.\n","\n","Even if you're a very experienced programmer, you should still take your time with this practice exercise to get used to coding in Python."]},{"cell_type":"markdown","metadata":{"id":"D5YxvanmBvKx"},"source":["#### Overview of Python syntax"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NjbnUzP5BvKx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408722205,"user_tz":-660,"elapsed":10,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"9047d2df-e620-48fd-a5f9-951ffc4f6a03"},"source":["# This is a comment\n","\n","# Python is dynamically typed, so data types are inferred at runtime\n","msg = \"Hello world\"  # string\n","x = 5  # int\n","y = 3.5  # float\n","z = True  # boolean\n","\n","# There is a 'None' keyword, which is similar to null/NULL in other languages\n","a = None\n","\n","# The standard division operator produces a float value\n","print(5 / 2)  # 2.5\n","# However, there's another operator if we want the result truncated\n","# This is called \"Integer divison\"\n","print(5 // 2)  # 2\n","\n","# Any number of variables can be printed at once\n","print(msg, x, y)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.5\n","2\n","Hello world 5 3.5\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"zwbeVZrXBvK2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408731827,"user_tz":-660,"elapsed":346,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"fb9563e6-9431-419f-9bcc-d14b631222f3"},"source":["# Code blocks are indicated using indentation instead of curly braces.\n","# If statements don't require parentheses.\n","if x == y:\n","    print(\"x has the same value as y\")\n","else:\n","    print(\"x and y have different values\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x and y have different values\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"KSfJ5uqMBvK5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408734980,"user_tz":-660,"elapsed":357,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"1db6b957-e71e-4e14-c732-401109af2144"},"source":["# Chained conditionals use \"elif\" instead of \"else if\"\n","if x < 0:\n","    print(\"x is negative\")\n","elif x > 0:\n","    print(\"x is positive\")\n","else:\n","    print(\"x is zero\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x is positive\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"MCHvu6WSBvK9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408736998,"user_tz":-660,"elapsed":345,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"1604e254-e637-4104-c4d2-0299dceca7c5"},"source":["# Boolean operators are the English words \"and\", \"or\", \"not\"\n","if x > 0 or y > 0:\n","    print(\"at least one of x and y are positive\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["at least one of x and y are positive\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"mgzMalOyBvLB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408739045,"user_tz":-660,"elapsed":331,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"b359e58a-ff01-406a-f158-764044590ed1"},"source":["# while loops don't require parentheses either\n","while x > 0:\n","    print(\"x equals\", x)\n","    x -= 1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["x equals 5\n","x equals 4\n","x equals 3\n","x equals 2\n","x equals 1\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"sr6lAvvvBvLF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408740913,"user_tz":-660,"elapsed":344,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"f2d136c2-5496-4981-f26d-1f347a220e51"},"source":["# Functions use the keyword \"def\"\n","def add_two_numbers(num_1, num_2):\n","    return num_1 + num_2\n","\n","summed = add_two_numbers(5, 3)\n","print(summed)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8\n"]}]},{"cell_type":"markdown","metadata":{"id":"YScjb54CBvLI"},"source":["#### Exercise 1\n","<font color='red'>In the next cell, write a function called `maximum` that takes two values and returns the larger of the two, using an if statement.</font>\n","\n","_Ensure that you re-run the cell containing your solution any time you make changes._"]},{"cell_type":"code","metadata":{"id":"Hujx3YLnBvLJ"},"source":["# TODO: Code your solution here\n","\n","# SOLUTION:\n","def maximum(l, r):\n","    if l > r:\n","        return l\n","    return r\n","\n","# Also valid\n","def maximum(l, r):\n","    return l if l > r else r"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CC4OEZF_BvLP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408746937,"user_tz":-660,"elapsed":333,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"36384d49-7b90-4b07-8261-3f45cc232905"},"source":["# Test your solution here\n","print(maximum(5, 3))   # should output 5\n","print(maximum(-5, 3))  # should output 3\n","print(maximum(0, 0))   # should output 0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","3\n","0\n"]}]},{"cell_type":"markdown","metadata":{"id":"gRJlo00XBvLS"},"source":["#### Python lists\n","\n","Python has two ordered data structures - `tuple` and `list`.\n","\n","Lists are the most frequently used of the two, as they are quite flexible and provide many functions to manipulate their entries."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"tHUwDIkbBvLT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408749853,"user_tz":-660,"elapsed":320,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"cd9fde7b-c16d-4059-d07e-22ff03b7c1ec"},"source":["# Create a list and append to it\n","my_list = [1, 2, 3]\n","my_list.append(4)\n","print(my_list)\n","\n","# The length of the list\n","print(len(my_list))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 4]\n","4\n"]}]},{"cell_type":"code","metadata":{"id":"M96dhW5gBvLW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408752716,"user_tz":-660,"elapsed":322,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"d2ab3fa8-466e-481c-ff6f-bca1c1966fce"},"source":["# Concatenate two lists\n","concat_list = [1, 2, 3] + [4, 5]\n","print(concat_list)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5]\n"]}]},{"cell_type":"code","metadata":{"id":"1uOS5wZSBvLZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408754452,"user_tz":-660,"elapsed":315,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"25904416-a2c2-4fb3-c107-8fdd7a2056c2"},"source":["# Indexing is zero-based\n","print(my_list[0], my_list[1])\n","\n","# Negative indexing counts from the end of the list\n","print(my_list[-1], my_list[-2])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1 2\n","4 3\n"]}]},{"cell_type":"code","metadata":{"id":"kXib--mBBvLd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408757143,"user_tz":-660,"elapsed":5,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"a1dccfec-777d-43d8-e739-213b3daf1764"},"source":["# Lists and tuples can be iterated over with the \"in\" keyword\n","for x in ['a', 'b', 'c']:\n","    print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a\n","b\n","c\n"]}]},{"cell_type":"markdown","metadata":{"id":"4Inwjq6-BvLg"},"source":["Tuples are immutable data structures, which means that their values can't be modified. They should be used where practical, as not only are they faster than lists, their immutability makes your code a little safer. For these reasons, you will mostly see tuples used for short sequences, particularly to describe things like the dimensions of an image.\n","\n","Their entries can be accessed in the same ways as lists like negative indexing, iterating, etc, so it's possible to write code which supports lists and tuples interchangeably - as we'll do in the next exercise."]},{"cell_type":"code","metadata":{"id":"X5UJeSW8BvLh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408761415,"user_tz":-660,"elapsed":511,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"12578d7e-20c0-425e-9198-f70176140828"},"source":["# Create a tuple. The difference in notation is that lists use square brackets\n","# and tuples use round parentheses\n","my_tuple = (1, 2, 3)\n","\n","print(my_tuple)\n","print(my_tuple[0])\n","\n","# Try to modify a value in-place and see what happens\n","# my_tuple[0] = -1\n","print(my_tuple)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 2, 3)\n","1\n","(1, 2, 3)\n"]}]},{"cell_type":"markdown","source":["Tuples can also be instantiated by just using commas. The parentheses are optional if it is otherwise unambiguous. This can be unexpected and confusing, so stick to using parentheses when creating a tuple. But it's worth knowing about."],"metadata":{"id":"w7l80tFeUnDg"}},{"cell_type":"code","source":["# Technically valid, but can be confusing\n","my_tuple2 = 1, 2, 3\n","# Note: default equality for tuples compares each element in the tuples\n","print(my_tuple == my_tuple2)\n","\n","# This leads to a difficult-to-diagnose error you should watch out for:\n","# the following instantiates a tuple, which is usually not what you wanted.\n","some_value = 10,"],"metadata":{"id":"NQWsanJxU09a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sTtyt3lYBvLk"},"source":["#### Exercise 2\n","<font color='red'>In the next cell, write a function called `arr_max` that takes a list (or tuple) and returns the largest element using a `for` loop and the `maximum` function you wrote in exercise 1. If the list is empty, return `None`.</font>\n","\n","_Ensure that you re-run the cell containing your solution any time you make changes._"]},{"cell_type":"code","metadata":{"id":"h4N4AaWiBvLl"},"source":["# TODO: Code your solution here\n","\n","# SOLUTION:\n","def arr_max(my_list):\n","    if len(my_list) == 0:\n","        return None\n","\n","    max_val = my_list[0]\n","    for val in my_list:\n","        max_val = maximum(max_val, val)\n","    return max_val\n","\n","# ALT SOLUTION\n","def arr_max(my_list):\n","    max_val = my_list[0] if my_list else None\n","\n","    for val in my_list:\n","        max_val = maximum(val, max_val)\n","    return max_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"GereXwJ4BvLo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408783125,"user_tz":-660,"elapsed":307,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"e319d546-817d-4c7e-c3ef-473ab4e302b6"},"source":["# Test your solution here\n","print(arr_max([-10, -5, 0, 5]))    # should output 5\n","print(arr_max((-10, -5, 0, 5)))    # should output 5 <-  tuple instead of list\n","print(arr_max([4, -1, 23, 6, -1])) # should output 23\n","print(arr_max([1, 1]))             # should output 1\n","print(arr_max([1]))                # should output 1\n","print(arr_max([]))                 # should output None"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","5\n","23\n","1\n","1\n","None\n"]}]},{"cell_type":"markdown","metadata":{"id":"tEO-Ck0jBvLq"},"source":["## Pytorch practice\n","Pytorch is a GPU-accelerated machine learning framework, which is nicely designed so that very little effort is required to run code on the GPU. We won't do GPU-enabled processing until the next lab, but you'll be surprised by just how easy it is!\n","\n","In this section we'll practice using Pytorch tensors, which are roughly equivalent to Python lists but with a whole lot of extra capabilities. Take your time going through the exercises and ensure that you understand what is happening. The rest of this subject depends on your understanding of Pytorch tensors, so extra effort put in now will be extremely useful with later labs!\n","\n","_Add comments anywhere you think will be helpful and try to keep the code neat. This way you can refer to this notebook when completing future labs._"]},{"cell_type":"code","metadata":{"id":"_sj3sP1vBvLr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408794856,"user_tz":-660,"elapsed":6009,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"cc4c122b-50ed-42a3-a000-698ab07a6fa2"},"source":["# Import Pytorch and print out its version\n","import torch\n","print(\"Pytorch version:\", torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pytorch version: 1.10.0+cu111\n"]}]},{"cell_type":"markdown","metadata":{"id":"7OwDMVpiBvLu"},"source":["#### Tensor basics"]},{"cell_type":"code","metadata":{"id":"WGCobGcOBvLv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408796618,"user_tz":-660,"elapsed":325,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"afcdc99c-c4dc-446c-b2a0-5b8fed9c1f4b"},"source":["# Tensors can be initialised with Python lists\n","my_list = [1.5, 2.5, 3.5]\n","my_tensor = torch.tensor(my_list)\n","\n","print(my_list, my_tensor)\n","\n","# This is a 1-dimensional tensor as it is just a flat list of values\n","print(len(my_list), len(my_tensor))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.5, 2.5, 3.5] tensor([1.5000, 2.5000, 3.5000])\n","3 3\n"]}]},{"cell_type":"code","metadata":{"id":"XmNcFzQABvLx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408798901,"user_tz":-660,"elapsed":313,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"23e3be4a-5cd3-4967-c381-d2aa91b25b48"},"source":["# Tensors allow for operations to be applied on the whole collection at once\n","print(my_tensor * 2)\n","\n","# What happens if we try the same thing on a python list? Uncomment the\n","# following line and run it to find out\n","# print(my_list / 2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3., 5., 7.])\n"]}]},{"cell_type":"code","metadata":{"id":"dzBVFTFDBvL3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408803401,"user_tz":-660,"elapsed":324,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"eed3fa17-1ee0-47fc-e1fc-278877b636ac"},"source":["# The data type is inferred when a tensor is initialised. You can access the\n","# data type of a tensor by using `dtype`\n","my_int_tensor = torch.tensor([1, 2, 3])\n","print(my_int_tensor.dtype)\n","\n","# To cast the tensor, use the \"type\" function or call one of the default \n","# typecast functions: .int() [for int32], .float() [for float32], .bool()\n","my_float_tensor = my_int_tensor.type(torch.float)\n","\n","print(my_float_tensor.int().dtype)\n","print(my_int_tensor.float().dtype)\n","print(my_int_tensor.bool().dtype)\n","print(my_float_tensor.dtype)\n","print(my_float_tensor)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.int64\n","torch.float32\n","tensor([1., 2., 3.])\n"]}]},{"cell_type":"code","metadata":{"id":"eQnEaTnrBvL6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408808835,"user_tz":-660,"elapsed":305,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"fc3c70b5-4e0e-4841-9527-a6626a5dad03"},"source":["# We can specify the dtype when initialising a tensor\n","my_float_tensor = torch.tensor([1, 2, 3], dtype=torch.float)\n","print(my_float_tensor.dtype)\n","\n","# Or we can use float values in our initialisation (by adding a decimal point)\n","my_float_tensor = torch.tensor([1., 2, 3])\n","print(my_float_tensor.dtype)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.float32\n","torch.float32\n"]}]},{"cell_type":"code","metadata":{"id":"zpsskFZ_BvL8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408811390,"user_tz":-660,"elapsed":318,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"afc019c1-ea67-411d-f4c5-67b059deec3e"},"source":["# Tensors can be operated on by a scalar\n","my_float_tensor = my_float_tensor * 2.5 + 3\n","\n","# Tensors can operate element-wise on one-another\n","another_tensor = torch.tensor([2, 4, 8])\n","\n","print(my_float_tensor + another_tensor)\n","print(my_float_tensor - another_tensor)\n","print(my_float_tensor * another_tensor)\n","print(my_float_tensor / another_tensor)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 7.5000, 12.0000, 18.5000])\n","tensor([3.5000, 4.0000, 2.5000])\n","tensor([11., 32., 84.])\n","tensor([2.7500, 2.0000, 1.3125])\n"]}]},{"cell_type":"markdown","metadata":{"id":"-h9G6WJ8BvMA"},"source":["#### Tensor initialisation"]},{"cell_type":"code","metadata":{"id":"bn5Mxeu6BvMB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408813503,"user_tz":-660,"elapsed":6,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"3378e0fe-0648-47d6-c65d-6eff185d2703"},"source":["# It's common to want a tensor full of zeros or ones.\n","# Pytorch provides helper functions where we simply give our desired size\n","ones_tensor = torch.ones(5)\n","zeros_tensor = torch.zeros(3)\n","print(ones_tensor, zeros_tensor)\n","\n","# We might also like a tensor of increasing numbers\n","range_tensor = torch.arange(5)\n","print(range_tensor)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1., 1.]) tensor([0., 0., 0.])\n","tensor([0, 1, 2, 3, 4])\n"]}]},{"cell_type":"code","metadata":{"id":"GSntTfPVBvMI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408816042,"user_tz":-660,"elapsed":2,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"7318156b-d2fe-4a8e-abed-8e5728da1e61"},"source":["# Another common tensor is the identity matrix\n","identity_matrix = torch.eye(3)\n","print(identity_matrix)\n","\n","# The identity matrix we created has two dimensions, meaning that it consists of\n","# both rows and columns. The number of rows and columns is called its \"shape\"\n","print(identity_matrix.shape)\n","\n","# We can make our own 2D matrix by manually initialising it with values. This\n","# tensor has two rows and three columns\n","my_matrix = torch.tensor([[0, 1, 2],\n","                          [3, 4, 5]])\n","print(my_matrix)\n","print(my_matrix.shape) "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n","torch.Size([3, 3])\n","torch.Size([2, 3])\n"]}]},{"cell_type":"code","metadata":{"id":"Jm-2rGHaBvMM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408818763,"user_tz":-660,"elapsed":304,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"dcf7a660-ab77-4d30-c9cf-0687932704c2"},"source":["# Tensors are stored in row-major form, which means the first dimension represents\n","# the rows in a tensor. Thus, indexing into the first dimension of the tensor will\n","# give us back an entire row\n","print(identity_matrix[0])\n","\n","# We can further index into this row to extract a specific value\n","print(identity_matrix[0][0])\n","\n","# Torch provides a way to index into multiple dimensions at once\n","print(identity_matrix[0, 0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 0., 0.])\n","tensor(1.)\n","tensor(1.)\n"]}]},{"cell_type":"code","metadata":{"id":"imNn-iFOBvMP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408821316,"user_tz":-660,"elapsed":6,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"e0af61ba-6d7d-4650-ba84-92908829436a"},"source":["# Let's create a three-dimensional tensor full of zeros\n","zeros_3d = torch.zeros((2, 3, 3))\n","print(zeros_3d.shape)\n","\n","# This tensor has a shape of (2, 3, 3) for a total of 18 values.\n","# We can imagine this as a list of two 3x3 matrices\n","print(zeros_3d)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 3])\n","tensor([[[0., 0., 0.],\n","         [0., 0., 0.],\n","         [0., 0., 0.]],\n","\n","        [[0., 0., 0.],\n","         [0., 0., 0.],\n","         [0., 0., 0.]]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"W0XNMM88BvMR"},"source":["#### Tensor reshaping\n","Tensors can have a shape made up of any number of dimensions, and we can reshape the tensors as we please - as long as it's a valid shape.\n","\n","For example, a completed Sudoku board contains 9x9=81 numbers arranged in a 9x9 matrix. However, these values can be arranged in a number of ways:\n","* a 9x9 grid (shape `(9, 9)`)\n","* a flat list of 81 numbers (shape `(81)`)\n","* three rows of 27 numbers (shape `(3, 27)`)\n","* 27 rows of three columns (shape `(27, 3`)\n","\n","All of these representations are valid, and still represent the same data. Torch allows for us to perform this sort of reshaping with a function called - you guessed it - `reshape`.\n"]},{"cell_type":"code","metadata":{"id":"dQw-1Madp7ds","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408831672,"user_tz":-660,"elapsed":300,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"5c19bbb5-3095-4d20-a66b-9322a00e3914"},"source":["# Let's initialise a 1D tensor with some values\n","range_tensor = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","print(range_tensor)\n","print(range_tensor.shape)\n","\n","# Another valid representation of this data is as a 3x3 matrix\n","range_mat = torch.tensor([[0, 1, 2],\n","                          [3, 4, 5],\n","                          [6, 7, 8]])\n","print(range_mat)\n","print(range_mat.shape)\n","\n","# Both of the above tensors have the same number of elements, which we can see\n","# by calling the \"numel\" function\n","print(range_tensor.numel(), range_mat.numel())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n","torch.Size([9])\n","tensor([[0, 1, 2],\n","        [3, 4, 5],\n","        [6, 7, 8]])\n","torch.Size([3, 3])\n","9 9\n"]}]},{"cell_type":"code","metadata":{"id":"Dkw6N58RBvMS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408835286,"user_tz":-660,"elapsed":330,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"36fc91e5-671c-4e15-c5b8-e9c701fa4242"},"source":["# First, let's create a tensor of numbers 0-8 and observe its shape\n","range_tensor = torch.arange(9, dtype=torch.float)\n","print(range_tensor, range_tensor.shape)\n","\n","# Let's convert this into a 3x3 matrix\n","range_mat = range_tensor.reshape(3, 3)\n","print(range_mat)\n","print(range_mat.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.]) torch.Size([9])\n","tensor([[0., 1., 2.],\n","        [3., 4., 5.],\n","        [6., 7., 8.]])\n","torch.Size([3, 3])\n"]}]},{"cell_type":"code","metadata":{"id":"pciA1SMQBvMV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408838993,"user_tz":-660,"elapsed":7,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"d3444bde-efee-4422-dc47-2aa56270a11e"},"source":["# We can reshape tensors without specifying one dimension (indicated by -1), and\n","# Torch is able to infer the missing dimension size\n","range_mat = range_tensor.reshape(-1, 3)\n","print(range_mat.shape)\n","\n","# We can also flatten the tensor to a single dimension using the same trick.\n","# It turns out flattening tensors is really useful for deep learning!\n","# Since fully connected layers only take 1D tensors.\n","# This will produce a 1D tensor of length 9\n","range_tensor = range_mat.reshape(-1)\n","print(range_tensor, range_tensor.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 3])\n","tensor([0., 1., 2., 3., 4., 5., 6., 7., 8.]) torch.Size([9])\n"]}]},{"cell_type":"code","metadata":{"id":"9h004T5zBvMY"},"source":["# Our reshaping has worked so far because we had the right number of elements.\n","# What happens if we try a shape that our data doesn't match? Uncomment each of\n","# the below lines and find out\n","# reshaped_tensor = range_tensor.reshape((3, 4))\n","# reshaped_tensor = range_tensor.reshape((-1, 4))\n","\n","# The two above reshape requests failed as 4 is not a factor of 9. I.e. 9/4\n","# doesn't give a whole number"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pn_-Ft62BvMb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408844039,"user_tz":-660,"elapsed":315,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"14328226-ffe0-4bf5-d61b-4c88988cf3fe"},"source":["# Let's modify a value in our original tensor\n","range_tensor[0] = -1\n","print(range_tensor)\n","print(range_mat)\n","\n","# We modified the original tensor, but values in the reshaped version were changed too!\n","# This is because when possible, reshape just creates a different \"view\" into the\n","# original tensor, so the tensors still share memory."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.])\n","tensor([[-1.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.]])\n"]}]},{"cell_type":"code","metadata":{"id":"o7v-QDbBBvMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408846584,"user_tz":-660,"elapsed":333,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"29930ff0-55a3-4755-9fb1-f5d1da3733bc"},"source":["# If we don't want them to share memory, we need to explicitly ask for it to be cloned\n","range_tensor_2 = range_tensor.clone()\n","range_tensor_2[0] = 99\n","print(range_tensor)\n","print(range_tensor_2)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-1.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.])\n","tensor([99.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.])\n"]}]},{"cell_type":"code","source":["# Two really common reshaping functions used in pytroch is called squeeze and\n","# and unsqueeze.\n","# Squeeze is used to remove dimensions of size one. Lets look at this example.\n","tensor_4D = torch.rand(1, 1, 2, 2)\n","print(\"Before squeeze:\")\n","print(tensor_4D.shape)\n","print(tensor_4D)\n","tensor_2D = tensor_4D.squeeze()\n","print(\"After squeeze:\")\n","print(tensor_2D.shape)\n","print(tensor_2D)\n","\n","# Unsqueeze is used to add a dimension to a tensor\n","tensor_1D = torch.rand(3)\n","print(\"Before unsqueeze:\")\n","print(tensor_1D.shape)\n","print(tensor_1D)\n","tensor_2D = tensor_1D.unsqueeze(0)\n","print(\"After unsqueeze:\")\n","print(tensor_2D.shape)\n","print(tensor_2D)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v9AAFECqbfIS","executionInfo":{"status":"ok","timestamp":1645418663292,"user_tz":-660,"elapsed":472,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"fb282358-2cc2-4151-d30f-cd602c0946cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Before squeeze:\n","torch.Size([1, 1, 2, 2])\n","tensor([[[[0.8380, 0.9787],\n","          [0.2090, 0.8108]]]])\n","After squeeze:\n","torch.Size([2, 2])\n","tensor([[0.8380, 0.9787],\n","        [0.2090, 0.8108]])\n","Before unsqueeze:\n","torch.Size([3])\n","tensor([0.3466, 0.3239, 0.0434])\n","After unsqueeze:\n","torch.Size([1, 3])\n","tensor([[0.3466, 0.3239, 0.0434]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Dz-5ICyxIn79"},"source":["#### Exercise 3\n","<font color='red'>In the next cell, reshape a 4D tensor shaped (2, 4, 2, 5) into a 2D tensor shaped (16,5) and store the result in `tensor_2D`.</font>\n","\n","Here we have combined the leading 3 dimensions into a single dimension. This is a common task in deep learning.\n","\n","_Bonus_: Passing a -1 into reshape tells Pytorch to figure out what that dimension should be. You can only include a single -1 per `reshape`. Try using -1 for the combined dimensions. "]},{"cell_type":"code","metadata":{"id":"0dUAxnvvJBmO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645421196355,"user_tz":-660,"elapsed":314,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"b993cf77-340c-408e-b3bd-405432206de4"},"source":["tensor_1D = torch.arange(80, dtype=torch.float)\n","tensor_4D = tensor_1D.reshape(2, 4, 2, 5)\n","print(tensor_4D)\n","\n","# TODO: fill in the code below to reshape the 4D tensor into a 2D tensor of shape (16, 5)\n","tensor_2D = tensor_4D.reshape(-1, 5)\n","\n","print(tensor_2D.shape)  # The shape should be 16, 5\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[ 0.,  1.,  2.,  3.,  4.],\n","          [ 5.,  6.,  7.,  8.,  9.]],\n","\n","         [[10., 11., 12., 13., 14.],\n","          [15., 16., 17., 18., 19.]],\n","\n","         [[20., 21., 22., 23., 24.],\n","          [25., 26., 27., 28., 29.]],\n","\n","         [[30., 31., 32., 33., 34.],\n","          [35., 36., 37., 38., 39.]]],\n","\n","\n","        [[[40., 41., 42., 43., 44.],\n","          [45., 46., 47., 48., 49.]],\n","\n","         [[50., 51., 52., 53., 54.],\n","          [55., 56., 57., 58., 59.]],\n","\n","         [[60., 61., 62., 63., 64.],\n","          [65., 66., 67., 68., 69.]],\n","\n","         [[70., 71., 72., 73., 74.],\n","          [75., 76., 77., 78., 79.]]]])\n","torch.Size([16, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZqHln4EDBvMg"},"source":["#### Tensor permute\n","\n","A very useful function in pytorch is the permute function which swaps dimensions around. This differs from reshape by keeping the same number of dimensions (E.g. from 4D tensor to 4D tensor) but swaps the orders of the dimensions lets look at an example. Permute actually moves the data from one memory location to another to end up with the desired shape. In contrast, reshape just re-interprets the memory block to say data from location a to location b now belongs to dimension x, etc.\n","\n","Note that even though the memory has ostensibly been swapped, pytorch just indexes into the same memory differently, and you still need to `clone` if you want new memory."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"cMT5IuUUBvMg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645421975760,"user_tz":-660,"elapsed":309,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"46864374-d267-4b76-adb1-a728c8f37540"},"source":["tensor_1D = torch.arange(120, dtype=torch.float)\n","tensor_4D = tensor_1D.reshape(2, 4, 3, 5)\n","# Swaps the first two dimensions and also the last two dimensions\n","tensor_4D_transposed = tensor_4D.permute((1, 0, 3, 2))\n","print(tensor_4D_transposed.shape)\n","\n","# The memory is not new\n","print(tensor_4D_transposed[0, 0, 0, 0])\n","tensor_4D_transposed[0, 0, 0, 0] = 999\n","print(tensor_4D[0, 0, 0, 0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 2, 5, 3])\n"]}]},{"cell_type":"markdown","source":["#### Exercise 4\n","<font color='red'>In the next cell, combine the last two dimensions of the 4D tensor and then move them to the first dimension.</font>\n","\n","The 4D tensor is shaped (2, 4, 3, 5), so in the end you will end up with a 3D tensor shaped (15, 2, 4). You will need to use both `reshape()` and `permute()` to accomplish this."],"metadata":{"id":"4mU6IyzMyyfh"}},{"cell_type":"code","source":["tensor_1D = torch.arange(120, dtype=torch.float)\n","tensor_4D = tensor_1D.reshape(2, 4, 3, 5)\n","\n","# TODO write code to combine the last two dimensions of the 4D tensor\n","# and then move the combined dimension to be the first dimension\n","\n","tensor_3D = tensor_4D.reshape(2, 4, -1).permute(2, 0, 1)\n","\n","print(tensor_3D.shape) #The shape should be (15, 2, 4)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CE_CNClE0MF2","executionInfo":{"status":"ok","timestamp":1645422676218,"user_tz":-660,"elapsed":5,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"4a3e9a99-ba78-4849-db4b-a3fa66d77852"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([15, 2, 4])\n"]}]},{"cell_type":"markdown","source":["#### Tensor slicing"],"metadata":{"id":"n2F5BPBOwFeC"}},{"cell_type":"code","metadata":{"id":"ipZU_FMUL91Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408900415,"user_tz":-660,"elapsed":307,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"4981f5d4-39a9-4b20-b7e7-6ed730b6e0ff"},"source":["# Python provides a way for us to take a slice from a\n","# list or tensor - instead of providing a single number as an index, we use two\n","# numbers separated by the colon : symbol\n","\n","# Let's create another range tensor to start with\n","range_tensor = torch.arange(10)\n","print(range_tensor)\n","\n","# Get the elements from index 3 to index 5 (not including index 5)\n","print(range_tensor[3:5])\n","\n","# Get the elements from index 0 to index 4 (not including index 4)\n","print(range_tensor[0:4])\n","\n","# If we don't wish to specify an end-point, we don't need to provide a value.\n","# Get all elements until index 4 (not including index 4)\n","print(range_tensor[:4])\n","\n","# Get all elements from index 4 (including index 4)\n","print(range_tensor[4:])\n","\n","# Using negative indexing, get the last 3 elements\n","print(range_tensor[-3:])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","tensor([3, 4])\n","tensor([0, 1, 2, 3])\n","tensor([0, 1, 2, 3])\n","tensor([4, 5, 6, 7, 8, 9])\n","tensor([7, 8, 9])\n"]}]},{"cell_type":"markdown","metadata":{"id":"mB-YRrGdBvMi"},"source":["#### Concatenation"]},{"cell_type":"code","metadata":{"id":"6JUra4y_BvMj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408905778,"user_tz":-660,"elapsed":306,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"c2129ebd-9dfc-47c1-9c89-d46f0e45051f"},"source":["# Python lists allow for concatenation like list1 + list2, but Torch interprets\n","# tensor1 + tensor2 as a summation. So we instead use the function torch.cat\n","tensor_a = torch.tensor([1, 2, 3, 4])\n","tensor_b = torch.tensor([5, 6, 7])\n","tensor_c = torch.cat((tensor_a, tensor_b))\n","print(tensor_c)\n","\n","# We can concatenate any number of tensors at the same time\n","tensor_d = torch.cat((tensor_a, tensor_a, tensor_b))\n","print(tensor_d)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3, 4, 5, 6, 7])\n","tensor([1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 7])\n"]}]},{"cell_type":"code","metadata":{"id":"4wMpBfxzBvMl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408909288,"user_tz":-660,"elapsed":308,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"55520ddb-66a5-43d6-f824-e44708e4a69e"},"source":["# Let's put together some dummy data.\n","# Each tensor will contain the age, height, and weight of a different person\n","alice_data = torch.tensor([36, 159, 68])\n","bob_data = torch.tensor([29, 174, 76])\n","print(alice_data, alice_data.shape)\n","print(bob_data, bob_data.shape)\n","\n","# Now we'll create a 2x3 matrix where each row is a person's data - we have two\n","# options for this.\n","# Option 1:\n","# - Convert each tensor into a 1x3 matrix\n","alice_data_1 = alice_data.reshape((1, -1))\n","bob_data_1 = bob_data.reshape((1, -1))\n","# - Concatenate the tensors along the first dimension\n","data_matrix_1 = torch.cat((alice_data_1, bob_data_1), dim=0)\n","print(data_matrix_1)\n","print(data_matrix_1.shape)\n","\n","# Option 2:\n","# - Use the stack function!\n","data_matrix_2 = torch.stack((alice_data, bob_data), dim=0)\n","print(data_matrix_2)\n","print(data_matrix_2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 36, 159,  68]) torch.Size([3])\n","tensor([ 29, 174,  76]) torch.Size([3])\n","tensor([[ 36, 159,  68],\n","        [ 29, 174,  76]])\n","tensor([[ 36, 159,  68],\n","        [ 29, 174,  76]])\n"]}]},{"cell_type":"code","metadata":{"id":"goC2RgqXBvMo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1602109683692,"user_tz":-660,"elapsed":5509,"user":{"displayName":"Ash Hall","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixYQ99U2CgtaJs0JqtxYt6mvv474jlU0CMjpglcrg=s64","userId":"13813718651376638511"}},"outputId":"e6a495b8-d2d0-49b1-fb37-7e432ba31093"},"source":["# torch.stack allows us to stack tensors along an arbitrary dimension, as long\n","# as the shapes are compatible. Thus, we can stack along the second dimension\n","# and produce the transpose of our other matrix!\n","print(torch.stack((alice_data, bob_data), dim=1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 36,  29],\n","        [159, 174],\n","        [ 68,  76]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3UgacTWyOpnu"},"source":["#### Exercise 5\n","<font color='red'>In the next cell, write a function called `add_person` that takes a tensor shaped `[n, 3]` where `n` is any integer (e.g. like `data_matrix_1` from the previous section), and a tensor `new_person` shaped `[3]` containing a person's age, height and weight. It should concatenate the new person to the end of the given matrix and return it.</font>\n","\n","Hint: you need to  turn the new_person tensor into a 2D matrix before you can concatenate it. You can not directly concatenate a 1D tensor onto the end of a 2D tensor. Look at the unsqueeze example we provided before to see how you can turn a 1D tensor into a 2D tensor.\n","\n","_Ensure that you re-run the cell containing your solution any time you make changes._"]},{"cell_type":"code","metadata":{"id":"R30uKXeEOqgU"},"source":["# TODO: Code your solution here\n","\n","# SOLUTION:\n","def add_person(people, new_person):\n","    print(\"The unsequeezed tensor looks like this: \", new_person.unsqueeze(0))\n","    return torch.cat((people, new_person.unsqueeze(0)), 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGDTzqFpPLBh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645415627815,"user_tz":-660,"elapsed":311,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"f2552b45-1916-44e5-81e1-5f899612144a"},"source":["# Test your solution here\n","charlie_data = torch.tensor([39, 166, 74])\n","dennis_data = torch.tensor([41, 183, 87])\n","\n","data_matrix_3 = add_person(data_matrix_1, charlie_data)\n","print(data_matrix_3)\n","# should output [[36, 159, 68],\n","#                [29, 174, 76],\n","#                [39, 166, 74]]\n","\n","data_matrix_3 = add_person(data_matrix_3, dennis_data)\n","print(data_matrix_3)\n","# should output [[36, 159, 68],\n","#                [29, 174, 76],\n","#                [39, 166, 74]\n","#                [41, 183, 87]]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The unsequeezed tensor looks like this:  tensor([[ 39, 166,  74]])\n","tensor([[ 36, 159,  68],\n","        [ 29, 174,  76],\n","        [ 39, 166,  74]])\n","The unsequeezed tensor looks like this:  tensor([[ 41, 183,  87]])\n","tensor([[ 36, 159,  68],\n","        [ 29, 174,  76],\n","        [ 39, 166,  74],\n","        [ 41, 183,  87]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"34s9iu_BBvMs"},"source":["#### Advanced indexing\n","To practice indexing into different dimensions of a tensor, let's make some dummy image data consisting of red, green and blue values. In Pytorch, this is represented as a 3-dimensional tensor, where the shape is `(3, height, width)`, where the 3 is for the colour channels R, G, and B."]},{"cell_type":"code","metadata":{"id":"TDKPQphhBvMu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1645408917804,"user_tz":-660,"elapsed":4,"user":{"displayName":"Zhen He","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07388169493391803483"}},"outputId":"151f01f1-e96b-4efc-ac2a-74ac98313940"},"source":["# So, let's create some random image-data!\n","num_channels = 3  # the colour channels R, G, B\n","height = 15\n","width = 30\n","\n","img = torch.rand(num_channels, height, width)\n","print(img.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 15, 30])\n"]}]},{"cell_type":"code","metadata":{"id":"i1GXz3lSSXj6"},"source":["# Before continuing, run this cell to import a library and set up a helper\n","# function to display images\n","import matplotlib.pyplot as plt\n","\n","def plt_img_tensor(tensor):\n","    \"\"\"Plot an image tensor using pyplot\"\"\"\n","    if tensor.ndim == 3:\n","        tensor = tensor.permute(1, 2, 0)\n","    plt.imshow(tensor.detach().cpu().numpy(), cmap='Greys_r')\n","    plt.clim(0, 1)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"BUWkDh9TBvMx","colab":{"base_uri":"https://localhost:8080/","height":215},"executionInfo":{"status":"ok","timestamp":1614325136797,"user_tz":-660,"elapsed":1319,"user":{"displayName":"Zhen He","photoUrl":"","userId":"07388169493391803483"}},"outputId":"9630e946-3e5b-41f1-f133-979a6b5faf83"},"source":["# Let's have a look at the beautiful image we created\n","plt_img_tensor(img)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"YapepBmoTSky","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"ok","timestamp":1614325138243,"user_tz":-660,"elapsed":1177,"user":{"displayName":"Zhen He","photoUrl":"","userId":"07388169493391803483"}},"outputId":"34c1de27-7732-4a27-df6c-d70315eddd3e"},"source":["# As the first dimension is the colour channels, we can view just the green channel\n","# by indexing into the first dimension\n","green_channel = img[1]\n","plt_img_tensor(green_channel)\n","print(green_channel.shape)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["torch.Size([15, 30])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ESzhmVqIBvM0"},"source":["#### Exercise 6\n","<font color='red'>In the next cell, store the blue channel of the image in a tensor called `blue_channel`.</font>\n","\n","_Ensure that you re-run the cell containing your solution any time you make changes._"]},{"cell_type":"code","metadata":{"id":"nCjreXh2BvM0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614325140702,"user_tz":-660,"elapsed":769,"user":{"displayName":"Zhen He","photoUrl":"","userId":"07388169493391803483"}},"outputId":"87072c49-370c-4085-a266-3f7d1931a0f0"},"source":["# TODO: Code your solution here\n","\n","# SOLUTION:\n","blue_channel = img[2]\n","print(blue_channel.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([15, 30])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TsW_8KU_BvM5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1602109684162,"user_tz":-660,"elapsed":5882,"user":{"displayName":"Ash Hall","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixYQ99U2CgtaJs0JqtxYt6mvv474jlU0CMjpglcrg=s64","userId":"13813718651376638511"}},"outputId":"63f37185-6df9-4be3-d604-1a8339ef08be"},"source":["# Test your solution here\n","print(blue_channel.shape == (height, width))  # Should output True\n","\n","# Note that the above test doesn't confirm that you have the right colour channel,\n","# only that you have *a* colour channel."],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"F88LCtmYBvM8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614325144895,"user_tz":-660,"elapsed":936,"user":{"displayName":"Zhen He","photoUrl":"","userId":"07388169493391803483"}},"outputId":"3f9b00f3-ad27-4db5-cafe-1cee929b0370"},"source":["# Let's try accessing values of specific pixels\n","# First, we'll access the green value of the top-left pixel (second colour-channel, first row, first column)\n","green_first_row = green_channel[0]\n","green_first_pixel = green_first_row[0]\n","print(green_first_pixel)\n","\n","# We can access the same information using a single line\n","print(img[1, 0, 0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(0.9503)\n","tensor(0.9503)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pxUM_YKdBvNA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614325146881,"user_tz":-660,"elapsed":807,"user":{"displayName":"Zhen He","photoUrl":"","userId":"07388169493391803483"}},"outputId":"e2e7a6c4-f142-492c-906f-725163088712"},"source":["# We've now seen how to access a row of green values, but how would we get all\n","# RGB values for an entire row? Torch allows us to skip indexing on a dimension,\n","# effectively saying \"give me everything in this dimension\".\n","\n","# Let's start with a simpler example before we return to the image.\n","# We'll begin with a 3x3 matrix\n","square_mat = torch.rand(3, 3)\n","print(square_mat)\n","\n","# We can easily take the first row\n","print(square_mat[0])\n","\n","# How do we take the first column? \n","# Remember how we saw the ':' symbol to take a slice? If we don't provide a start\n","# or end index, we actually ask for *all* of the values in that dimension!\n","\n","# So here we use the symbol to take everything in the first dimension, then\n","# choose an index in the second (column) dimension. This gives us a column\n","print(square_mat[:, 0])\n","\n","# Note that it will appear as a row when printed to the screen, but the\n","# important thing is that we have the values of the column we wanted."],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0.8483, 0.3618, 0.5887],\n","        [0.9066, 0.3573, 0.3920],\n","        [0.4713, 0.3451, 0.4211]])\n","tensor([0.8483, 0.3618, 0.5887])\n","tensor([0.8483, 0.9066, 0.4713])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TODORYImBvNE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614325152791,"user_tz":-660,"elapsed":774,"user":{"displayName":"Zhen He","photoUrl":"","userId":"07388169493391803483"}},"outputId":"7c928330-4c53-468a-94f6-af816b2a1300"},"source":["# Back to the image example, how would we extract the first row of RGB values?\n","# Think about it in terms of what each dimension represents\n","# The first dimension is the colour channels - we want all of them (:)\n","# The second dimension is the rows - we want the first row (0)\n","# The third dimension is the columns - we want all of them (:)\n","rgb_row = img[:, 0, :]\n","print(rgb_row.shape)\n","\n","# Great! We have a 3x30 matrix, which is num_channels x width.\n","# However, we can write it in a slightly simpler way - we don't need to specify that we want everything\n","# in the last dimension, as this is the default behaviour.\n","rgb_row = img[:, 0]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([3, 30])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cH70lC3RBvNJ"},"source":["<font color='red'>The next cell, extract the RGB values of the first column of the image and store it in a variable called `rgb_col`.</font>\n","\n","_Ensure that you re-run the cell containing your solution any time you make changes._"]},{"cell_type":"code","metadata":{"id":"EicS3TJJBvNJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614325178784,"user_tz":-660,"elapsed":754,"user":{"displayName":"Zhen He","photoUrl":"","userId":"07388169493391803483"}},"outputId":"9656aa8e-9a49-42b4-8ebc-bf92cc36d2ec"},"source":["\n","rgb_col = img[:, :, 0]\n","print(rgb_col.shape == (num_channels, height))  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cXHVE54pBvNQ"},"source":["#### Exercise 7\n","<font color='red'>In the next cell, extract the RGB values of the *last* column of the image and store it in a variable called `rgb_col`. </font> \\\n","\n","_Hint_: the solution is really similar to the previous cell. Remember _negative_ indexing?\n","\n","_Ensure that you re-run the cell containing your solution any time you make changes._"]},{"cell_type":"code","metadata":{"id":"L0sxb49oBvNQ"},"source":["# TODO: Code your solution here\n","\n","# SOLUTION\n","rgb_col = img[:, :, -1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lCp8NtxGBvNU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1602109684169,"user_tz":-660,"elapsed":5808,"user":{"displayName":"Ash Hall","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixYQ99U2CgtaJs0JqtxYt6mvv474jlU0CMjpglcrg=s64","userId":"13813718651376638511"}},"outputId":"bcec2f5f-531c-4986-b438-24de7b4552e3"},"source":["# Test your solution here\n","print(rgb_col.shape == (num_channels, height))  # Should output True\n","\n","# Note that the above test doesn't confirm that you have the right column, only that you have *a* column."],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EXwm_A1-BvNY"},"source":["#### Exercise 8 (Challenge)\n","<font color='red'>In the next cell, add another row of random RGB data to the image, and store the new image in a variable called `new_img`.</font> \\\n","You'll need to:\n","- Figure out the required shape of the new row\n","- Create random values of that shape\n","- Concatenate the new values to the image\n","\n","_Ensure that you re-run the cell containing your solution any time you make changes._"]},{"cell_type":"code","metadata":{"id":"AQB5jJITBvNY"},"source":["# TODO: Code your solution here\n","\n","# SOLUTION\n","new_row = torch.rand(num_channels, 1, width)\n","new_img = torch.cat((img, new_row), 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8AJW2PhBvNf","colab":{"base_uri":"https://localhost:8080/","height":243},"executionInfo":{"status":"ok","timestamp":1602109684502,"user_tz":-660,"elapsed":6124,"user":{"displayName":"Ash Hall","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GixYQ99U2CgtaJs0JqtxYt6mvv474jlU0CMjpglcrg=s64","userId":"13813718651376638511"}},"outputId":"28866483-2198-4244-9a1b-e0d79f20979a"},"source":["# Test your solution here\n","print(new_img.shape == (num_channels, height + 1, width))  # Should output True\n","\n","# View the new image\n","plt_img_tensor(new_img)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}