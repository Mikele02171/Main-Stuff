{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0lAszzUtMGKS"},"source":["# Lab 5 - Debugging Neural Networks\n","\n","In this notebook we'll take a look at three common issues encountered when building and training neural networks. As a lot of the code has been written for you, work slowly through the notebook, reading along the way - there is some important information!"]},{"cell_type":"markdown","metadata":{"id":"mdocimy3CAuL"},"source":["## Data Normalisation\n","\n","In this exercise we'll take a look at a form of [normalisation](https://en.wikipedia.org/wiki/Normalization_(statistics)) and how it can aide the training of neural networks (and other optimisation techniques, for that matter). As mentioned in the lectures, neural networks are most stable when the inputs and outputs have values near zero, with a standard deviation of approximately 1. Let's see what this looks like in practice.\n","\n","We will demonstrate the importance of normalisation using an interesting forecast prediction dataset. In this dataset we will use cartographic variables of small regions of forest (30m x 30m) to predict the type of forecast cover in each region. Example cover types include: spruce/fir (type 1), lodgepole pine (type 2), etc.\n","\n","You can get more information about the data from this link.\n","https://archive.ics.uci.edu/ml/datasets/covertype\n","\n","There is also a kaggle competition for this dataset:\n","https://www.kaggle.com/c/forest-cover-type-prediction/overview\n","\n","According to the kaggle competition the best team got an accuracy of 100% on the test set! \n"]},{"cell_type":"code","source":["pip install torchmetrics"],"metadata":{"id":"8tcHT-b2XBqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLyNJBZIkkru"},"source":["# Quickly set up our device\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","device = torch.device(\"cpu\")\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","    torch.cuda.set_device(device)\n","print(\"Training on\", device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QK6ca8tVulaf"},"source":["Here is the code we use to load the data into a pandas dataframe."]},{"cell_type":"code","metadata":{"id":"tGjSlxSFcYKy"},"source":["import pandas as pd\n","\n","# Read the data from csv file\n","cover_type_df = pd.read_csv('https://github.com/zhenhe1/datasets/raw/5852941fa9cd21326d3f7277d971e46e48ea2714/covtype_dataset.zip')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OaSY9go5uso2"},"source":["Lets take a look at the data using the next cell. Each row of the data contains the data for one small 30m x 30m region of the forest. You will notice the cartographic variables include things like elevation, aspect, slope, soil types etc. You will notice that the soil type values are much smaller than elevation, aspect, hydrology measurements. This data will need to be normalised, ensuring all input features are **approximately** in the range `[-1,1]`. There are two reasons for normalisation:\n","\n","1. If the average magnitude of one feature is higher than another, then that feature is implicitly more important during early epochs (the model might eventually learn around this and give that feature a lower weight). By normalising, you ensure that each feature is treated equally at all times and the model can choose which features are important.\n","2. Loss functions and default learning rates are designed assuming your inputs and outputs are normalised.\n","\n","\n","We will first try to train a network without normalisation and then, later, add normalisation and see what happens.\n"]},{"cell_type":"code","metadata":{"id":"r3lRrn6YcY4X"},"source":["cover_type_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bg2gYvwswGek"},"source":["<a name='create-dataset'></a>\n","### Create a dataset class\n","In the next cell we will create a dataset class for our forest cover data. Notice this time the dataset class will directly take a pandas dataframe and then extract the input and labels from it. The data has 55 columns, the first 54 columns contain the cartographic variables used to predict forecast cover type. Notice we convert the data into numpy arrays instead of PyTorch tensors. It is fine for datasets to output numpy arrays since we will wrap a dataloader around the dataset, and the dataloader will automatically convert the numpy arrays into PyTorch tensors.\n","\n","<font color=red>Complete the `CovDataset` class below, except for the last `TODO`. This will be completed later.</font>"]},{"cell_type":"code","metadata":{"id":"iomKGo5WelaO"},"source":["# Import the base class\n","from torch.utils.data import Dataset\n","\n","# Subclassing in Python just requires adding the class name in parentheses\n","class CovDataset(Dataset):\n","    # The __init__ method is similar to a constructor like you find in other\n","    # languages. \n","    def __init__(self, df):\n","        # We directly take the pandas data frame split the data into the inputs\n","        # and labels.\n","        # The first 54 columns contain the input attributes\n","        # The last column called Cover_type contain the data we want to predict\n","        self.inputs = df[df.columns[:54]].to_numpy().astype(np.float32)\n","        self.labels = df.Cover_Type.to_numpy().astype(np.int64)\n","        # We make a deep copy of the inputs and store it as a copy of the original.\n","        # This way when we normalize the data in a future exercise we can make sure\n","        # we always apply the normalization based on the original inputs.\n","        self.original_inputs =  self.inputs.copy()\n","\n","    def __len__(self):\n","        # TODO: Return the length (number of items) in the dataset\n","\n","\n","    def __getitem__(self, idx):\n","        # TODO: Store the inputs and labels for the item at index idx in variables\n","        #       input and label\n","        # input = ...\n","        # label = ...\n","        \n","      \n","        return input, label\n","\n","    def normalise_data(self, mean, std):\n","        pass\n","        # TODO: There will be instructions to complete this function later.\n","        #       For now, SKIP THIS TODO.\n","        #       When it is time to implement this method:\n","        #         1. Implement the Standard score. \n","        #            i.e. subtract the mean and divide by the std.\n","        #         2. Remember to only change self.inputs, \n","        #            not self.original_inputs!\n","\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PfcSj7Mdxwt4"},"source":["<a name=\"create-splits\"></a>\n","### Create train, validation and test dataset splits and dataloaders\n","\n","Notice the difference between this labs data loading code versus lab2a. The main difference is that we created a dataset class in the previous cell and passed the dataframe into it. So all the messy extraction of input data and labels are done in the dataset class. Writing a dataset class is considered a more elegant way to code dataloading. So we encourage you to do that in the future as well.\n","\n","You will notice the this dataset is quite large.\n","\n","<font color=red>In the next cell complete the code to create a Dataset object, split it into train/validation/test and finally create a DataLoader for each Dataset.</font>\n","\n"]},{"cell_type":"code","metadata":{"id":"9-O1HG9shUo3"},"source":["from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n","\n","# The percentages for each partition\n","TRAIN_SPLIT = 0.8\n","VAL_SPLIT = 0.1\n","TEST_SPLIT = 0.1\n","BATCH_SIZE = 256\n","\n","# Ensure that the splits add to 100%\n","assert TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT == 1\n","\n","# TODO : create the forecast cover dataset (CovDataset) object using the cover_type_df\n","#        dataframe.\n","# entireCoverDataset = \n","\n","\n","# Calculate the number of examples in each partition\n","train_size = int(TRAIN_SPLIT * len(entireCoverDataset))\n","val_size = int(VAL_SPLIT * len(entireCoverDataset))\n","test_size = len(entireCoverDataset) - train_size - val_size\n","\n","print(\"Train examples:     \", train_size)\n","print(\"Validation examples:\", val_size)\n","print(\"Test examples:      \", test_size)\n","\n","\n","#TODO Just like you did in lab2a please use the random_split function to \n","#     break entireCoverDataset into train_dataset, val_dataset, test_dataset\n","# train_dataset, val_dataset, test_dataset = \n","\n","\n","\n","\n","#TODO Just like you did in lab2a please create the train_loader, val_loader\n","#     and test_loader using their corresponding datasets.\n","#     Please do random shuffling for the train_loader, do not do random shuffle\n","#     for the other two data loaders\n","\n","# train_loader =\n","# val_loader =\n","# test_loader = \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wfcp-KX0lgs4"},"source":["Lets take a look at the first element of the training dataset that we created. You should see a tuple where the first element contains a 1D array with 54 elements and the second element stores the output label."]},{"cell_type":"code","metadata":{"id":"rCLUadTukD2L"},"source":["print(train_dataset[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X4lAnCZr5q1M"},"source":["### MLP model\n","\n","In the cell below finish the definition of a MLP model that will be used in the training loop. Notice we have already specified the default input and output size for the model. \n","\n","<font color=red>In the next cell, add the requested layers to the `nn.Sequential()`</font>"]},{"cell_type":"code","metadata":{"id":"IU_e7ShVl9vw"},"source":["\n","# Custom models need to subclass nn.Module\n","class MLP(nn.Module):\n","    def __init__(self,\n","                 device,\n","                 input_size=54, # default to 54\n","                 output_size=8 # default to the number of classes\n","                 ):\n","        super().__init__()\n","\n","        # Write the classifier layers here.\n","        self.seq = nn.Sequential(\n","            \n","            ## TODO: add the following layers\n","            #        Linear layer that outputs 64 neurons\n","            #        ReLU\n","            #        Linear layer that outputs 64 neurons\n","            #        ReLU\n","            #        Linear layer that outputs output_size neurons\n","\n","        \n","        )\n","        # Transfer the model weights to the GPU\n","        self.to(device)\n","    \n","    # The forward method takes input values and returns predictions. We just need\n","    # to pass the inputs through the layers we defined in __init__\n","    def forward(self, x):\n","        return self.seq(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9zQhj8H6bIj"},"source":["### Training loop\n","To keep things simple we have just copied the training loop used in lab 2b here to do the training. Note we are not using the validation dataset in order to keep the training code short. We have set the optimizer to the Adam optimizer with initial learning rate of 0.001.\n","\n","Lets train the model and see what happens."]},{"cell_type":"code","metadata":{"id":"5CGeH6MXnmda"},"source":["import torch.optim as optim\n","from tqdm.notebook import tqdm\n","import torchmetrics\n","\n","criterion = nn.CrossEntropyLoss()\n","model = MLP(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","#initialize metric\n","train_accuracy = torchmetrics.Accuracy(task = 'multiclass', num_classes = 8).to(device)\n","\n","# The number of times we loop over the entire dataset\n","total_epochs = 10\n","for epoch in tqdm(range(total_epochs)): \n","    epoch_train_accuracy = []\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)   \n","        acc = train_accuracy(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    print('epoch: %d loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n","    print('epoch: %d accuracy: %.3f' % (epoch + 1, train_accuracy.compute()))\n","    # reset for the next epoch\n","    train_accuracy.reset()\n","print('Finished Training')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9d-l6Oq7Evm"},"source":["If everything goes according to plan you will notice that the training accuracy is around 77%. This is not that high considering other people can get 100% accuracy. Lets see what happens when we try it on the test dataset."]},{"cell_type":"code","metadata":{"id":"H-DfqSIEqi1c"},"source":["running_loss = 0.0\n","test_accuracy = torchmetrics.Accuracy(task = 'multiclass', num_classes = 8).to(device)\n","\n","for i, data in enumerate(test_loader, 0):\n","   inputs, labels = data\n","   inputs, labels = inputs.to(device), labels.to(device)\n","\n","   model.eval()\n","   with torch.no_grad():\n","    outputs = model(inputs)\n","    loss = criterion(outputs, labels)\n","    running_loss += loss.item()\n","    acc = test_accuracy(outputs, labels) \n","\n","print(\"test loss: \", running_loss/len(test_loader))\n","print('test accuracy: ', test_accuracy.compute())\n","# reset internal state to make the metric ready for new data\n","test_accuracy.reset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YB-AGEhU7lkK"},"source":["### Normalising the data\n","The test dataset also just reports accuracy of around 77%. Lets see if using normalisation can significantly improve the results.\n","\n","There are several forms of normalisation. The normalisation we will do is called the [Standard score](https://en.wikipedia.org/wiki/Standard_score#Calculation) or z-score. To calculate the Standard score, you subtract the mean and divide by the standard deviation. So this means we need to compute the mean and standard deviation of every column of the data resulting in **54 means** and **54 standard deviations**. \n","\n","Doing this normalisation to each column will make the values in each column have similar range of values and make them small and centered around zero. This will improve training by ensuring the inputs have similar magnitudes and thus are weighted equally, and will better match the default learning rate of Adam.\n","\n","The purpose of a validation/testing set is to test generalisation: i.e. to check how well the model does on unseen examples. You must never use any part of the testing set to tune your model, else you are no longer correctly testing generalisation. Thus, we will calculate the mean and standard deviation from only the training set and then apply it to all splits equally. If we were to deploy a model, then we would use the mean/std from the training set to normalise the inputs there, too.\n","\n","<font color=red>In the next code cell extract the numpy array containing the input data from the **training** dataset and calculate the means/stds of **all** columns.</font>\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"gK-JE8AFJ8EM"},"source":["# Here we extract the input data of the training dataset into a\n","# numpy array called train_inputs.\n","train_inputs = entireCoverDataset.inputs[train_dataset.indices]\n","\n","# TODO : write code to extract the mean and standard deviation for each column \n","#        of the data and store them in the following variables.\n","#        train_inputs_means = \n","#        train_inputs_stds = \n","\n","\n","assert train_inputs_means.shape == (54,)\n","assert train_inputs_stds.shape == (54,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will now use these means and standard deviations (stds) to normalise our data. \n","\n","<font color=red>\n","\n","1. Go back to `CovDataset` in [Create a dataset class](#create-dataset) and add a function called `normalise_data`.\n","2. Re-run the cell in the [Create train, validation and test dataset splits and dataloaders](#create-splits).\n","3. Run the next cell to call the `normalise_data` function you added to the CovDataset object.\n","\n","</font>\n","\n","*Note: The `random_split` function does not copy the data into the train/validation/test datasets. Instead, they are views into the `entireCoverDataset`. Thus we only need to normalise `entireCoverDataset`.*\n","\n","Once you have normalised the data, please go back to the training loop and execute it again. See how much higher the training and test accuracy goes. You should see a very significant increase in the accuracy (around 10% higher accuracy).\n","\n","If you have time at the end of the lab. Feel free to come up with better models and also try different optimisation parameter values to see if you can push the accuracy even higher."],"metadata":{"id":"bmf7YEBjY7pV"}},{"cell_type":"code","source":["# We call the normalise_data function on the entire data set.\n","# This normalises all the input data for the train, validation and test splits\n","# in one go. \n","# TODO: Please go to the CovDataset class and add a function called\n","#       normalize_data which normalises the input by subtracting the mean\n","#       and dividing by the standard deviation from the original_input.\n","entireCoverDataset.normalise_data(train_inputs_means, train_inputs_stds)"],"metadata":{"id":"Fw1kWqOeY5qK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZqLNhhWUUKYQ"},"source":["## Learning Rate and Class Imbalance\n","\n","In the past, we've mentioned the effects of a poor choice of learning rate, and we'll now look at a practical example.\n","\n","To demonstrate this, we'll build a simple image dataset consisting of white squares and rectangles on a black background, with the task of classifying them as either squares or rectangles."]},{"cell_type":"markdown","metadata":{"id":"Wlh4uTUNeonM"},"source":["### Preparation\n","\n","The dataset has been implemented you. Feel free to read through it - although the implementation isn't particularly important.\n","\n","Each example in the dataset consists of a white rectangle of a randomly chosen width and height, with a label of `1` if it's a square and `0` if it's a rectangle."]},{"cell_type":"code","metadata":{"id":"gVzxCZsVS32B"},"source":["from torch.utils.data import Dataset\n","\n","class ToyImageDataset(Dataset):\n","    def __init__(self, num_examples):\n","        super().__init__()\n","        height, width = 12, 12\n","        self.images, self.labels = self.gen_examples(num_examples, height, width)\n","\n","    def gen_examples(self, num_examples, height, width):\n","        # Rectangles can be at most half the width or height of the image\n","        max_height, max_width = height // 2, width // 2\n","\n","        # Initialise a tensor of zeros to fill in with rectangles\n","        images = torch.zeros((num_examples, 1, height, width), dtype=torch.float32)\n","        #images = torch.zeros((num_examples, 1, height, width), dtype=torch.int32)\n","\n","        # Randomly generate the top-left corner of each rectangle\n","        tops = torch.randint(0, height - 1, (num_examples,))\n","        lefts = torch.randint(0, width - 1, (num_examples,))\n","        # Randomly generate the heights and widths\n","        heights = torch.randint(1, max_height + 1, (num_examples,))\n","        widths = torch.randint(1, max_width + 1, (num_examples,))\n","        # Calculate the bottom-right corner of each rectangle, being sure to not\n","        # go beyond the image edge\n","        bottoms = torch.clamp(tops + heights, 0, height)\n","        rights = torch.clamp(lefts + widths, 0, width)\n","\n","        # Colour in the rectangles\n","        for i, (t, l, b, r) in enumerate(zip(tops, lefts, bottoms, rights)):\n","            images[i, :, t:b, l:r] = 1\n","\n","        # Squares are rectangles with equal widths and heights\n","        labels = (heights == widths).type(torch.float32).unsqueeze(-1)\n","        #labels = (heights == widths).type(torch.int32).unsqueeze(-1)\n","        return images, labels\n","\n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self, idx):\n","        return self.images[idx], self.labels[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_aojIUOth0lL"},"source":["Run the next cell to create the datasets and dataloaders."]},{"cell_type":"code","metadata":{"id":"cpMhiYA2MFZY"},"source":["from torch.utils.data import DataLoader\n","torch.manual_seed(42)\n","\n","BATCH_SIZE = 64\n","\n","train_dataset = ToyImageDataset(900)\n","test_dataset = ToyImageDataset(100)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fSr5aNQRh3rJ"},"source":["Let's take a look at some examples from the training dataset. You'll notice that there are very few square examples - we'll return to this later."]},{"cell_type":"code","metadata":{"id":"TVwhfIITcePL"},"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 8))\n","for i, (image, label) in enumerate(train_dataset):\n","    if i == 24: break\n","    plt.subplot(4, 6, i+1)\n","    plt.imshow(image[0], cmap='gray', interpolation='nearest')\n","    plt.axis('off')\n","    plt.title('Square' if label else 'Rect')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AkU-XqXgiLhO"},"source":["The CNN for this task is implemented below - note that as we're doing *binary* classification (between two classes), we output a single value. A value less than `0.5` is a prediction of class `0` and a value greater than `0.5` is a predictions of class `1`."]},{"cell_type":"code","metadata":{"id":"xa7-MFw3gs7n"},"source":["import torch.nn as nn\n","\n","# This Printer class is just used to print out the shape of the tensor before\n","# putting it into the linear layer so we know what is the input size for the linear\n","# layer.\n","class Printer(nn.Module):\n","    def forward(self, x):\n","        print(x.shape)\n","        return x\n","\n","class ConvNet(nn.Module):\n","    def __init__(self, device):\n","        super().__init__()\n","        self.seq = nn.Sequential(\n","            nn.Conv2d(1, 2, 3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(2, 4, 3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(4, 4, 3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(4, 4, 3, padding=1),\n","            nn.ReLU(),\n","            # Printer(),\n","            nn.Flatten(),\n","            nn.Linear(36, 1)\n","        )\n","        self.to(device)\n","\n","    def forward(self, x):\n","        return self.seq(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ziZ8ow_CN-bu"},"source":["This function plots training and testing values (we'll use it to graph the loss and accuracy)."]},{"cell_type":"code","metadata":{"id":"jMWyjXNfN9nA"},"source":["def plot(title, train_vals, test_vals):\n","    plt.plot(range(len(train_vals)), train_vals, label='Train')\n","    plt.plot(range(len(test_vals)), test_vals, label='Test')\n","    plt.legend()\n","    plt.title(title)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PhvAMyP9Ngm5"},"source":["Here we have a function to perform training and testing."]},{"cell_type":"code","metadata":{"id":"rr2sTAJMENBX"},"source":["from tqdm.notebook import tqdm\n","\n","def train_and_test(model, train_loader, test_loader, loss_func, optimiser, num_epochs, accuracy=None):\n","    train_losses, test_losses = [], []\n","    train_accs, test_accs = [], []\n","    for epoch in tqdm(range(num_epochs)):\n","        sum_loss, sum_acc = 0, 0\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimiser.zero_grad()\n","            outputs = model(inputs)\n","            loss = loss_func(outputs, labels)\n","            loss.backward()\n","            optimiser.step()\n","            sum_loss += loss.item()\n","            if accuracy: acc = accuracy(outputs, labels.type(torch.int32))\n","        train_losses.append(sum_loss / len(train_loader))\n","        train_accs.append(accuracy.compute().cpu())\n","        if accuracy: accuracy.reset()\n","\n","        sum_loss, sum_acc = 0, 0\n","        for inputs, labels in test_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = loss_func(outputs, labels)\n","            sum_loss += loss.item()\n","            aaa = labels.type(torch.int32)\n","            if accuracy: acc = accuracy(outputs, labels.type(torch.int32))\n","        test_losses.append(sum_loss / len(test_loader))\n","        test_accs.append(accuracy.compute().cpu())\n","        if accuracy: accuracy.reset()\n","    return train_losses, test_losses, train_accs, test_accs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"apE5PWd2iqzz"},"source":["<a name=\"train-model\"></a>\n","### Train the Model\n","\n","We now call the training and plotting functions to train and view the training and testing accuracy curves."]},{"cell_type":"code","metadata":{"id":"yz4wxeq7fMOB"},"source":["import torchmetrics\n","\n","torch.manual_seed(42)\n","model = ConvNet(device)\n","# When predicting a probability (e.g. probability that the image is of a square)\n","# it's relatively common for the last layer of the network to be nn.Sigmoid(),\n","# as this ensures the outputs are between 0 and 1.\n","# However, Binary Cross Entropy Loss expects the input to be logits, which means\n","# we should not use nn.Sigmoid(), as this loss function already does that. \n","loss_func = nn.BCEWithLogitsLoss()\n","\n","\n","\n","# TODO: First try the code. Then adjust the learning rate (lr) to something more reasonable like 1e-3.\n","optimiser = torch.optim.Adam(model.parameters(), lr=1)\n","\n","num_epochs= 1000 \n","\n","accuracy=torchmetrics.Accuracy(task = 'binary', threshold=0.5).to(device)\n","\n","train_losses, test_losses, train_accs, test_accs = train_and_test(model,\n","                                                  train_loader,\n","                                                  test_loader,\n","                                                  loss_func,\n","                                                  optimiser,\n","                                                  num_epochs,\n","                                                  accuracy)\n","\n","plot(\"Accuracy\", train_accs, test_accs)\n","\n","print(\"train loss: \", train_losses[-1])\n","print(\"train accuracy: \", train_accs[-1])\n","print(\"test loss: \", test_losses[-1])\n","print(\"test accuracy: \", test_accs[-1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkcFKl5Oiwwa"},"source":["### Fix the Learning Rate Problem\n","\n","That train acurracy is very unstable! Also the test accuracy just stays flat  This sort of accuracy curve is usually indicative of a badly-selected learning rate. The learning rate above is set to `1`, which would be considered a very high learning rate.\n","\n","\n","<font color='red'>Adjust the learning rate and train the model again. Try something smaller like 1e-3.</font>"]},{"cell_type":"markdown","metadata":{"id":"RKKQl5-7Wv1s"},"source":["### Detecting the class imbalance problem\n","\n","The test accuracy looks weird, it starts at 80% and then only moves to around 85%. Lets now look a bit further under the hood. By looking at the distribution of rectangle and square examples in the train and test sets."]},{"cell_type":"code","metadata":{"id":"Vwj35PkWh3QJ"},"source":["train_square_proportion = len(torch.nonzero(train_dataset.labels)) / len(train_dataset.labels)\n","train_rect_proportion = 1 - train_square_proportion\n","\n","test_square_proportion = len(torch.nonzero(test_dataset.labels)) / len(test_dataset.labels)\n","test_rect_proportion = 1 - test_square_proportion\n","\n","print(\"Train data - Square: \", len(torch.nonzero(train_dataset.labels)))\n","print(\"Train data - Rectangle: \", len(train_dataset.labels)- len(torch.nonzero(train_dataset.labels)))\n","print(\"Train data - Rectangle proportion:\", \"{:.3f}\".format(train_rect_proportion), \"Square proportion:\", \"{:.3f}\".format(1- train_rect_proportion))\n","print(\"Test data - Rectangle proportion:\", \"{:.3f}\".format(test_rect_proportion), \"Square proportion:\", \"{:.3f}\".format(1- test_rect_proportion))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GSHRyol9kBzT"},"source":["It turns out the reason the model initially got 80% accuracy was because it just always predicted rectangle no matter what input it got. Since there was 80% rectangles in the test set it got the answer correct 80% of the time. Later the model was able to sometimes predict Squares but still not often enough hence it did not move much higher than 80%. There are so many more examples of rectangles compared to squares so during backprop most of the gradients try to steer the model towards predicting rectangles. \n","\n","Lets take a look at the confusion matrix on the trained model to confirm our intuition that the model is predicting too many rectangles instead of squares.\n","\n","The first cell below creates a function that will plot a confusion matrix with some defaults.\n","\n","The second cell contains the code that uses the `plot_confusion_matrix` function defined in the first cell to draw our confusion matrix."]},{"cell_type":"code","metadata":{"id":"zOod1ZyalKaQ"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import ConfusionMatrixDisplay\n","\n","def plot_confusion_matrix(cm, classes, normalise=False, title='Confusion matrix', cmap=plt.cm.Blues):\n","    if normalise:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalised confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalisation')\n","    \n","    fig, ax = plt.subplots()\n","    cmd = ConfusionMatrixDisplay(cm, display_labels=classes)\n","    fmt = '.2f' if normalise else 'd'\n","    cmd.plot(cmap=plt.cm.Blues, ax=ax, values_format=fmt)\n","    ax.set_title(title)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6YSuw50l-_E"},"source":["Our code to draw the confusion matrix of our model results"]},{"cell_type":"code","metadata":{"id":"VVOb7FhBXBoc"},"source":["from torchmetrics import ConfusionMatrix\n","\n","# Initialize metric\n","metric = ConfusionMatrix(task = 'binary').to(device)\n","\n","for inputs, labels in test_loader:\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    outputs = model(inputs)\n","    metric(outputs, labels.type(torch.int32))\n","\n","cm = metric.compute().detach().cpu().numpy()\n","plot_confusion_matrix(cm, [\"rectangle\", \"square\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xme8Ga6AnDOY"},"source":["The bottom left and top right cells are where the predictions are different from the true labels. This result looks not too bad. It seems we are making a bit more mistakes predictions when the ground truth is squares (bottom left cell) than when the ground truth is rectangles (top right cell). But the problem looks much more obvious when we look at the next cell where we plot the normalised confusion matrix. "]},{"cell_type":"code","source":["plot_confusion_matrix(cm, [\"rectangle\", \"square\"], normalise=True)"],"metadata":{"id":"Ub9pH6pRmkGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7TZ4VoxknllT"},"source":["**This result shows us a really big problem!** Look at the bottom left cell. It says that we wrongly predicted about 55% (you may get something between 45% to 70%) of the images that contained squares. So this really shows us our result is actually much worse than the around 85% accuracy reported earlier. We need to use a different metric that better reflects our true performance in terms of both predicting rectangles and squares. So a fairer metric that treats rectangles and squares equally.\n","\n","So lets turn to the unweighted average recall (UAR) metric which we discussed in lecture 4 to help us out. The UAR metric essentially assigns equal importance to correctly predicting rectangles and squares when they are present in the image. \n","\n","The next cell computes the UAR performance of our model on the test set.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"tCRilq5xZ86o"},"source":["import numpy as np\n","from torchmetrics import Recall\n","\n","#initialize metric\n","# We are using multiclass recall rather than binary recall since binary recall in TorchMetrics\n","# does not support UAR and also does not allow us to print the recall for each class separately.\n","recall_perclass = Recall(task = 'multiclass', num_classes = 2, average='none').to(device)\n","recall_uar = Recall(task='multiclass', num_classes = 2, average='macro',).to(device)\n","# Gather targets and predictions for the whole test set\n","targets, predictions = [], []\n","for inputs, labels in test_loader:\n","    inputs, labels = inputs.to(device), labels.to(device)\n","    outputs = model(inputs)\n","    # Note this code is used to convert the binary outputs to a separate predicted probability\n","    # for rectangles and squares since the multiclass recall requires separate outputs per class\n","    # so the shape goes from (64, 1) to (64, 2)\n","    # Normally this line is not needed if your model already outputs multiple classes probabilities\n","    outputs = torch.cat((1.0- outputs, outputs), 1)\n","    labels = labels.squeeze(1)\n","    recall = recall_perclass(outputs, labels.type(torch.int32))\n","    uar = recall_uar(outputs, labels.type(torch.int32))\n","\n","recall_rects, recall_squares =  recall_perclass.compute().detach().cpu().numpy()\n","print(\"recall for rectangles =\", recall_rects)\n","print(\"   recall for squares =\", recall_squares)\n","print(\"                  UAR =\", recall_uar.compute().detach().cpu().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1g6nUfRic0tQ"},"source":["Notice the recall for squares is much lower than the recall for the rectangles. The UAR (the average of the two recalls) is about 0.7 (the number you get maybe different should be mostly similar). Lets try some techniques to help balance the training."]},{"cell_type":"markdown","metadata":{"id":"RDcaK4vjwNMb"},"source":["### Fix the Class Imbalance Problem: WeightedRandomSampler\n","\n","Now that we have a method for truly measuring the model's performance, we should fix the class imbalance.\n","\n","The simplest technique for correcting this is to change the proportion of examples that we present to the model. If we randomly select examples without any consideration for the imbalance, we will show the model 80% rectangles and 20% squares on average - we would like to instead show it 50% of each.\n","\n","PyTorch provides a class [WeightedRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler) to help us with this. Instead of giving each example the same chance of being selected from the dataset, we can weight their selection probability - so an example with a weight of `2` is twice as likely to be selected as an example with a weight of `1`.\n","\n","To undo the imbalance, we choose the weights based on the proportion of examples. So if classes `A` and `B` have a 9:1 ratio in a dataset, we would need to weight class `A` by a factor of 1, and class `B` by a factor of 9.\n","\n","To compute this directly, the weights of a class should be $1 - p$, where $p$ is the proportion of that class in the dataset. We have already computed the proportions for each of our two classes, so we can use this equation to compute their appropriate weights.\n","\n","<font color='red'>In the next cell, compute the sampling weights for the two classes.</font>"]},{"cell_type":"code","metadata":{"id":"Uml4iiS0vKJs"},"source":["# TODO: Compute the square and rectangle weights using the above equation\n","# square_weight = ...\n","# rect_weight = ...\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GuwDCend5b20"},"source":["The [WeightedRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler) class requires a weight for each example in the dataset, so we will iterate over every label in the training dataset and build a list of the correct weight. Run the next cell to do so."]},{"cell_type":"code","metadata":{"id":"mHULDTTE5Tx7"},"source":["weights = [square_weight if l == 1 else rect_weight for l in train_dataset.labels]\n","\n","# Print out the first 5 weights\n","print(weights[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cJzdTi5353-3"},"source":["All that's left now is to initialise the sampler and use it to create a new train dataloader.\n","\n","In the next cell, initialise a `torch.utils.data.sampler.WeightedRandomSampler` by providing the weights list and the length of the train dataset as arguments.\n","\n","If your code is correct then the number of squares should be roughly half of the number of examples in the training set."]},{"cell_type":"code","metadata":{"id":"kT_YxAwL52wW"},"source":["# TODO: Initialise a WeightedRandomSampler\n","# sampler = ...\n","\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n","\n","total = 0\n","for inputs, labels in train_loader:\n","  total = total + labels.detach().cpu().numpy().sum()\n","\n","print(\"number of squares: \", total)\n","print(\"total number of training examples:\", len(train_dataset))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BIAq3HBQ64lM"},"source":["Once you've completed the above cell, return to the [Train the Model](#train-model) section to try it out.\n","\n","Once training has completed, run the cell that computes the UAR. If all goes well, the square recall should and the UAR should be higher than before as well. This is because we are showing the model balanced porportions of squares and rectangles throughout training. However, don't be too stressed if the square recall and UAR does not go higher. We have noticed there is some randomness in the results due to the small number of different square examples used during training. You may need to run again to see the benefits. The majority of the times when you rerun the training with the balanced sampler the results should be better. But you might get unlucky once or twice.\n","\n","Although this method is very useful for handling class imbalance, it can't perform magic - there is a limit to its efficacy. So your UAR will still not be that high. There are other ways to handle skewed distributions. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"He9ky8tsqTDp"},"source":["### Fix the Class Imbalance Problem: weighted loss function\n","\n","There is another way to address skewed training distributions. We can increase the size of the gradients going back for training examples of the minority class. This way we can train in a more balanced way without the need to adjust the sampling rate of the two classes. \n","\n","The loss that we are currently using is called `torch.nn.BCEWithLogitsLoss`. It turns out that it already has a parameter called`pos_weight` that we can use to adjust the weighting we assign to the positive class (the class with label 1).\n","\n","Take a look at this documentation to work out how you can modify the loss so the minor class gets more weight :\n","https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n","\n","It is up to you to decide what weight to use and how to code it. Good luck! You should be able to get better results than using the balanced sampling method.\n","\n","<font color=red>\n","\n","1. Run the next cell to reset the dataloader so that it doesn't use WeightedRandomSampler\n","2. Determine what weights to use and modify the cell in [Train the Model](#train-model) accordingly\n","\n","</font>\n"]},{"cell_type":"code","metadata":{"id":"re7PcLDItF8G"},"source":["# Run this code to reset the dataloader so we do not use the weighted sampler\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4wVkrtxMpuK4"},"source":["## Challenge\n","\n","As a challenge try to change the model, learning rate, optimization algorithms, increase the number of training epochs, play around with the loss weight, etc. to see if you can improve the UAR score, and let us know the best UAR you can get! Note that due to the small number of square training examples, you might find a lot of random fluctuation in your test accuracy which makes optimization very hard, so don't spend too much time on this. You should keep a record of what you tried in an excel spreadsheet like we did in lab 3. This is a really important habit to form since people tend to forget what they tried after some time.\n","\n","You can also play around and see if you can optimize the accuracy of the forest cover classification problem as well. This is a much larger dataset and so produces more stable results. Also for fun you might want to see if the class distribution of this dataset is also imbalanced. If so, you want to do something about that too!\n","\n","Good luck! By the end of this lab you will have practiced optimizing many different neural network models across the last few labs! You are starting to turn into a neural network optimizer pro!"]},{"cell_type":"markdown","metadata":{"id":"2dsoZL-L7xwe"},"source":["That's it for this lab - nice work! Hopefully you've learnt something about debugging neural networks. Although there are many ways in which a neural network can fail, we've now seen three of the most common problems."]}]}