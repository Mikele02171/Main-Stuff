{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"5fsCX5JKBvKu"},"source":["# Lab 2b - Pytorch Basics\n","\n","In this notebook we will convert the problem we solved in the lab02a notebook into a classification problem instead. So we will assign each wine quality rating to a different class. So quality of 0 will belong to a different class to quality of 1.\n","\n","You will need to copy over some of the code you wrote for data loading from the lab02a notebook into this notebook.\n","\n","The biggest difference between a regression solution and multi-class classification in terms of implementation is that now our model needs to output 10 values (1 for each quality rating) instead of just a single output. Also we will need to change our loss function."]},{"cell_type":"code","metadata":{"id":"W8AJW2PhBvNf"},"source":["# Import Pandas\n","import pandas as pd\n","\n","# There are two datasets available, but we'll just work with the larger, white\n","# wine dataset. Feel free to play around with the red wine dataset once you've\n","# finished the lab\n","red_wine_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n","white_wine_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n","\n","# It's a single function call to load a dataset. CSV files typically use commas\n","# as delimiters between records, but our dataset uses semicolons so we had to\n","# specify it with the \"delimiter\" argument.\n","all_data = pd.read_csv(white_wine_url, delimiter=';')\n","\n","print(type(all_data))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HrvUipMNt06C"},"source":["You will need to copy the contents of the cell starting with <font color = red> target_column = \"quality\" </font> from the lab02a notebook into this notebook. "]},{"cell_type":"code","metadata":{"id":"mcEDRSKjrI18"},"source":["target_column = \"quality\"\n","\n","# TODO: Extract just the *input* features. Instead of specifying all of the features we\n","# want, we should drop the feature we *don't* want.\n","# x_data = ...\n","\n","\n","\n","# TODO: Extract the target feature. We don't want a Series here, but a DataFrame\n","# with one column (see the above cell)\n","# y_data = ...\n","\n","\n","# If your implementation is correct the shape of y_data should be (4898, 1)\n","# So y_data is a 2D tensor containing 4898 examples and just 1 feature.\n","print(\"y_data shape:\", y_data.shape) \n","\n","# Just like with tensors, we can print the shape\n","num_examples = x_data.shape[0]\n","num_input_features = x_data.shape[1]\n","\n","# If your implementation is correct the number of samples should be 4898\n","print(\"Number of examples:\", num_examples) \n","\n","# If your implementation is correct the number of input features should be 11\n","print(\"Number of input features:\", num_input_features)\n","\n","# If your implementation the shape of the x_data tensor should be (4898, 11)\n","# which means it is a 2D array where each row represents one example and each\n","# column represents one feature\n","print(\"x_data shape:\", x_data.shape )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZwPbJpHej5S"},"source":["You will need to copy the contents of the cell starting with <font color = red> # Import Torch and the dataset utilities we need </font> from the lab02a notebook into this notebook. "]},{"cell_type":"code","metadata":{"id":"R4dK-21abmtk"},"source":["# Import Torch and the dataset utilities we need\n","import torch\n","from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n","\n","# The percentages for each partition\n","TRAIN_SPLIT = 0.8\n","VAL_SPLIT = 0.1\n","TEST_SPLIT = 0.1\n","# Ensure that the splits add to 100%\n","assert TRAIN_SPLIT + VAL_SPLIT + TEST_SPLIT == 1\n","\n","\n","# TODO: Create two tensors and initialise them with x_data.values and y_data.values.\n","# The dtype should be torch.float32. DataFrame.values directly returns the 2D\n","# data in the dataframe, which is what Torch requires to initialise a tensor.\n","# x_tensor = ...\n","# y_tensor = ...\n","\n","\n","# Now we construct a TensorDataset - a simple class used to associate each x and\n","# y value in our tensors.\n","full_dataset = TensorDataset(x_tensor, y_tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R199K4j-e2HM"},"source":["You will need to copy the contents of the cell starting with <font color = red> # Calculate the number of examples in each partition\n"," </font> from the lab02a notebook into this notebook. "]},{"cell_type":"code","metadata":{"id":"wX2E5L1Fx3p3"},"source":["# Calculate the number of examples in each partition\n","train_size = int(TRAIN_SPLIT * len(all_data))\n","val_size = int(VAL_SPLIT * len(all_data))\n","test_size = len(all_data) - train_size - val_size\n","\n","print(\"Train examples:     \", train_size)\n","print(\"Validation examples:\", val_size)\n","print(\"Test examples:      \", test_size)\n","\n","# Before we actually split the dataset, we seed Torch's random number generator.\n","# This ensure that we end up with the exact same partitions every time it's run.\n","torch.manual_seed(42)\n","\n","# TODO: Split the dataset using the random_split function we imported earlier.\n","# The function takes a dataset and a list of partition lengths.\n","# Hint: We already have all of these variables available\n","# train_dataset, val_dataset, test_dataset = random_split(...)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-XI39wwe-i6"},"source":["## Create the DataLoaders\n","\n","You will need to copy the contents of the create dataloaders cell from lab02a notebook into this cell. The cell starts by setting <font color=red>BATCH_SIZE = 64</font>"]},{"cell_type":"code","metadata":{"id":"z3NZrKhIeTvB"},"source":["# When you've finished the lab, try modifying the batch size to see what effect\n","# it has on your results\n","BATCH_SIZE = 64\n","\n","# TODO: Construct a DataLoader for each Dataset. The constructor takes three\n","# arguments - a Dataset, the batch size, and a boolean indicating whether it\n","# should shuffled. We will set shuffle=True for train dataloader.\n","\n","# train_loader = DataLoader(...\n","# val_loader = DataLoader(...\n","# test_loader = DataLoader(...\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojUURBDh9-hm"},"source":["### Enable GPU Training *(if available)*\n","The rise in popularity of deep learning is largely a result of the availability of good Graphics Processing Units. So although it's not required, it's definitely good to utilise a GPU if you can.\n","\n","It's exceptionally easy to use a compatible GPU in Pytorch - we can do it in just a few lines of code!"]},{"cell_type":"code","metadata":{"id":"2co2qquU7lcR"},"source":["# By default we'll assume that GPU acceleration isn't available\n","device = torch.device(\"cpu\")\n","\n","# Check if GPU acceleration is available (requires a CUDA-compatible GPU) and\n","# set the device variable accordingly. If the computer has more than one GPU,\n","# you can specify which one by replacing 0 with a different index\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\")\n","    torch.cuda.set_device(device)\n","\n","print(\"Training on\", device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRbspMJc-Egx"},"source":["### Define the Model\n","Here you will need to change the MLP you defined in lab02a notebook so that it outputs 10 features instead of just 1. Since to perform classification we need to output a value for each of the 10 quality classes (from 0 to 9)."]},{"cell_type":"code","metadata":{"id":"SM8YqMMz0kCE"},"source":["# Import the neural network module of Pytorch. We access its methods like \"nn.Linear\"\n","import torch.nn as nn\n","\n","# Our model class must subclass nn.Module\n","class MLP(nn.Module):\n","    # The __init__ method is similar to a constructor like you find in other\n","    # languages. We will take the device as an argument to transfer the model to the GPU\n","    def __init__(self, device):\n","        super().__init__()\n","        # TODO: Initialise a Sequential module consisting of the below layers, and\n","        # store it in the member variable self.seq\n","        #  - a linear layer mapping from num_input_features to 20 hidden features\n","        #  - a ReLU activation layer\n","        #  - a linear layer mapping from 20 hidden features to a 10 features (the wine quality)\n","        # You can look here for an example:\n","        #     https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n","        # self.seq = nn.Sequential(...\n","\n","        \n","    \n","        # The model stays on the CPU by default. Calling the \"to\" method transfers\n","        # the model weights to whichever device we specified\n","        self.to(device)\n","\n","    # Our forward method simply takes the input batch x, passes it through our\n","    # Sequential module, and returns the outputs (predictions)\n","    def forward(self, x):\n","        return self.seq(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Example of how to use torch metrics compute different metrics\n"],"metadata":{"id":"uBmRPfQeExVf"}},{"cell_type":"code","source":["#This installs the torchmetrics library\n","!pip install torchmetrics"],"metadata":{"id":"VtW9SPL9E_Ws"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchmetrics\n","\n","# Create the Accuracy metric\n","accuracy = torchmetrics.Accuracy(task = 'multiclass', num_classes = 5).to(device)\n","\n","batch_size = 10\n","for i in range(batch_size) :\n","\n","    # simulate a classification problem\n","    predictions = torch.randn(10, 5).softmax(dim=-1).to(device)\n","    target = torch.randint(5, (10,)).to(device)\n","\n","    # training step accuracy\n","    acc = accuracy(predictions,target)\n","    print(f\"Accuracy on batch {i}: {acc}\")\n","\n","# total accuracy over all training batches\n","print(f\"Accuracy on all data: {accuracy.compute()}\")\n","\n","# reset internal state to make the metric ready for new data\n","accuracy.reset()"],"metadata":{"id":"TwDBzkRCtVe9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["*Note*: The internal state needs to be reset between epochs and should not be mixed across training, validation, and testing. It is therefore highly recommend to re-initialize the metric per mode as shown above."],"metadata":{"id":"WJq-rX3qtrBV"}},{"cell_type":"markdown","source":["TorchMetrics' API lets you define your own custom metrics by simply subclassing torchmetrics.Metric and implementing the following methods: \n","1. `init()`: Each state variable should be called using self.add_state(…). \n","2. `update()`: Any code needed to update the internal metrics states for accumulation given any inputs to the metric. \n","3. `compute()`: Computes a final value from the state of the metric.\n","\n","The below code snippet implements custom metric of Root Mean Square Error "],"metadata":{"id":"hMtTfHbzuUkc"}},{"cell_type":"code","source":["class RMSE(torchmetrics.Metric):\n","\n","  def __init__(self, dist_sync_on_step=False):\n","      super().__init__(dist_sync_on_step=dist_sync_on_step)\n","      # dist reduce fx indicates the function that should be used to\n","      # reduce state from multiple processes\n","      self.add_state( \"sum_squared_errors\", torch.tensor(0), dist_reduce_fx=\"sum\")\n","      self.add_state(\"n_observations\", torch.tensor(0), dist_reduce_fx=\"sum\")\n","\n","  def update(self, preds, target):\n","      # update metric states\n","      self.sum_squared_errors += torch.sum((preds - target)**2)\n","      self.n_observations += preds.numel()\n","\n","  def compute( self):\n","      # compute final result\n","      return torch.sqrt(self.sum_squared_errors / self.n_observations)\n","\n","\n","target = torch.tensor([2, 5, 4, 8])\n","preds = torch.tensor([3, 5, 2, 7])\n","# Test that it gives the same answer as TorchMetrics\n","our_rmse_metric = RMSE()\n","our_rmse = our_rmse_metric(preds, target)\n","print(\"Our RMSE:   \", \"{:.3f}\".format(our_rmse))\n","\n","their_rmse_metric = torchmetrics.MeanSquaredError(squared=False)\n","their_rmse = their_rmse_metric(preds, target)\n","print(\"Their RMSE: \", \"{:.3f}\".format(their_rmse))"],"metadata":{"id":"ce2Wwx3F6tyY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JVlR_DGsuITZ"},"source":["## Simple Training loop\n","\n","This is currently the same simple training loop as used for lab02a. You will need to make the following changes to make it work for classification:\n","\n","\n","1.   Change the <font color=red>loss</font> function to <font color=red>nn.CrossEntropyLoss()</font>.\n","2.   Currently the labels are 2D tensors of shape \\[Batch size, 1\\] we need to change it to 1D tensors of shape \\[Batch size\\]. We can do this using the squeeze function (<font color=red>torch.squeeze(tensor_name)</font>). \n","3.   Currently the labels have float32 type. We need to change it to type long to make it work for classification. The way to convert a tensor into a type long is to use the long function (<font color=red>tensor_name.long()</font>). \n","4.   Use torchmetrics library to compute the classification accuracy.\n","\n"]},{"cell_type":"code","source":["import torch.optim as optim\n","import numpy as np\n","\n","# TODO: Change this loss to use the correct loss for classification\n","#criterion = nn.MSELoss()\n","\n","\n","# Use MLP as the model\n","model = MLP(device)\n","# Use the SGD optimizer with initial learning rate set to 0.0001\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n","# The number of times we loop over the entire dataset\n","total_epochs = 100\n","\n","for epoch in range(total_epochs):  # loop over the dataset multiple times\n","\n","    # TODO: create the Accuracy metric object\n","    # train_accuracy = ....\n","\n","\n","    # The following is computed in a single pass through the dataset\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","\n","        # TODO: Write here code that coverts the labels tensor from shape [batchsize, 1] to shape [batchsize]\n","        #       Next write code that converts the labels tensor to type long().\n","        #       labels = ....\n","   \n","\n","        \n","\n","        # Copy the data to the specified device\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward + backward + optimize\n","        outputs = model(inputs)\n","\n","        # Compute the loss using the loss function\n","        loss = criterion(outputs, labels)\n","\n","        # TODO: write code here for computing the classification accuracy\n","        #       by using torchmetrics library\n","        #       acc = train_accuracy(....)\n","\n","        \n","        \n","        # Perform backprop using the loss\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    # print statistics\n","    print('epoch: %d loss: %.3f' % (epoch + 1, running_loss / len(train_loader)))\n","    \n","    # TODO: report the total accuracy over all training batches\n","    # print('epoch: %d accuracy: %.3f' % (epoch + 1, TODO HERE ))\n","    \n","\n","\n","print('Finished Training')"],"metadata":{"id":"mEzLEtXWwUlu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkWu_koZTbHb"},"source":["<font color = red> Isn't it so much cooler to see an accuracy measure rather than just loss! It actually lets us know how close we are to 100%. This is one of the benefits of doing classification rather than regression. There is no clear accuracy metric for regression."]},{"cell_type":"markdown","metadata":{"id":"W_4VO_ZI6h8c"},"source":["## Simple Testing loop\n","\n","Now modify the testing loop so that is also works for classification. Do all the things you did for the training loop with the exception of changing the loss function.\n"]},{"cell_type":"code","metadata":{"id":"crJwn4Z56iTE"},"source":["running_loss = 0.0\n","test_accuracy = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10).to(device)\n","\n","for i, data in enumerate(test_loader, 0):\n","   # get the inputs; data is a list of [inputs, labels]\n","   inputs, labels = data\n","  \n","   # TODO: Write here code that coverts the labels tensor from shape [batchsize, 1] to shape [batchsize]\n","   #       Next write code that converts the labels tensor to type long().\n","   #       labels = ...\n","   #       labels = ...\n","\n","   \n","\n","   # Copy the data to the specified device\n","   inputs, labels = inputs.to(device), labels.to(device)\n","\n","   model.eval()\n","   with torch.no_grad():\n","    # Forward + backward + optimize\n","    outputs = model(inputs)\n","\n","    # Compute the loss using the loss function\n","    loss = criterion(outputs, labels)\n","    running_loss += loss.item()\n","\n","    # TODO: write code here for computing the classification accuracy\n","    #       by using torchmetrics library\n","    #       acc = test_accuracy(...)\n","\n","    \n","\n","print(\"test loss: \", running_loss/len(test_loader))\n","\n","# TODO: write code here for computing the test accuracy\n","#       print('test accuracy: ', TODO Here)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IAc35TPd4joF"},"source":["### Make the results better!\n","Look at some of the things you tried for improving the regression problem result and see how well they work for classification. What is the highest classification accuracy you can get? "]},{"cell_type":"code","metadata":{"id":"AuigNCdraELY"},"source":[],"execution_count":null,"outputs":[]}]}